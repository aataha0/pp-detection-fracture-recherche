{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNqItx6quu7O7VzwUw/6S5Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/botatooo/pp-detection-fracture-recherche/blob/dev/src/fracatlas_efficientdet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaehsA6usXtQ",
        "outputId": "d5940326-3ae1-4676-c84e-5ddae530bb2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/facebookresearch/detectron2.git\n",
            "  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-req-build-8mjquw5e\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-8mjquw5e\n",
            "  Resolved https://github.com/facebookresearch/detectron2.git to commit 864913f0e57e87a75c8cc0c7d79ecbd774fc669b\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (9.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (3.7.1)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.0.7)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.4.0)\n",
            "Collecting yacs>=0.1.8 (from detectron2==0.6)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (0.9.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.2.1)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (4.66.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.15.1)\n",
            "Collecting fvcore<0.1.6,>=0.1.5 (from detectron2==0.6)\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting iopath<0.1.10,>=0.1.7 (from detectron2==0.6)\n",
            "  Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
            "Collecting omegaconf<2.4,>=2.1 (from detectron2==0.6)\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hydra-core>=1.1 (from detectron2==0.6)\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting black (from detectron2==0.6)\n",
            "  Downloading black-23.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (23.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (1.23.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (6.0.1)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from hydra-core>=1.1->detectron2==0.6)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting portalocker (from iopath<0.1.10,>=0.1.7->detectron2==0.6)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (4.47.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (2.8.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (8.1.7)\n",
            "Collecting mypy-extensions>=0.4.3 (from black->detectron2==0.6)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting pathspec>=0.9.0 (from black->detectron2==0.6)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (4.1.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (2.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (4.5.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.60.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.5.1)\n",
            "Requirement already satisfied: protobuf<4.24,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (2.31.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard->detectron2==0.6) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.6) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->detectron2==0.6) (3.2.2)\n",
            "Building wheels for collected packages: detectron2, fvcore, antlr4-python3-runtime\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, detectron2\n",
        "\n",
        "!nvcc --version\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
        "print(\"detectron2:\", detectron2.__version__)\n"
      ],
      "metadata": {
        "id": "Ynemyv6St0De"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from torchvision.datasets.utils import download_and_extract_archive\n",
        "\n",
        "root = \"dataset/\"\n",
        "url = \"https://figshare.com/ndownloader/files/41725659\"\n",
        "filename = \"fracatlas.zip\"\n",
        "\n",
        "# if download:\n",
        "if not os.path.isdir(os.path.join(root, \"FracAtlas\")):\n",
        "    os.makedirs(root, exist_ok=True)\n",
        "    download_and_extract_archive(\n",
        "        url,\n",
        "        os.path.dirname(root),\n",
        "        filename=filename,\n",
        "        remove_finished=True,\n",
        "    )\n",
        "if not os.path.isdir(root):\n",
        "    raise RuntimeError(\n",
        "        \"Dataset not found or corrupted. You can use download=True to download it\"\n",
        "    )\n",
        "\n",
        "with open(\"dataset/FracAtlas/Annotations/COCO JSON/COCO_fracture_masks.json\") as f:\n",
        "  fracture_masks_data = json.load(f)\n",
        "\n",
        "fractured_images = [i[\"file_name\"] for i in fracture_masks_data[\"images\"]]\n",
        "fractured_image_count = len(fractured_images)\n",
        "\n",
        "training_images = fractured_images[: int(0.9 * fractured_image_count)]\n",
        "testing_images = fractured_images[int(0.9 * fractured_image_count) :]\n",
        "\n",
        "\n",
        "os.mkdir(\"data\")\n",
        "os.mkdir(\"data/fracatlas\")\n",
        "\n",
        "\n",
        "os.mkdir(\"data/fracatlas/images\")\n",
        "\n",
        "os.mkdir(\"data/fracatlas/images/train\")\n",
        "for i in training_images:\n",
        "  full_path = os.path.abspath(os.path.join(\"dataset/FracAtlas/images/Fractured\", i))\n",
        "  new_path = os.path.abspath(os.path.join(\"data/fracatlas/images/train\", i))\n",
        "  os.rename(full_path, new_path)\n",
        "\n",
        "os.mkdir(\"data/fracatlas/images/val\")\n",
        "for i in testing_images:\n",
        "  full_path = os.path.abspath(os.path.join(\"dataset/FracAtlas/images/Fractured\", i))\n",
        "  new_path = os.path.abspath(os.path.join(\"data/fracatlas/images/val\", i))\n",
        "  os.rename(full_path, new_path)\n",
        "\n",
        "\n",
        "os.mkdir(\"data/fracatlas/labels\")\n",
        "\n",
        "os.mkdir(\"data/fracatlas/labels/train\")\n",
        "for i in training_images:\n",
        "  i = i.replace(\".jpg\", \".txt\")\n",
        "  full_path = os.path.abspath(os.path.join(\"dataset/FracAtlas/Annotations/YOLO\", i))\n",
        "  new_path = os.path.abspath(os.path.join(\"data/fracatlas/labels/train\", i))\n",
        "  os.rename(full_path, new_path)\n",
        "\n",
        "os.mkdir(\"data/fracatlas/labels/val\")\n",
        "for i in testing_images:\n",
        "  i = i.replace(\".jpg\", \".txt\")\n",
        "  full_path = os.path.abspath(os.path.join(\"dataset/FracAtlas/Annotations/YOLO\", i))\n",
        "  new_path = os.path.abspath(os.path.join(\"data/fracatlas/labels/val\", i))\n",
        "  os.rename(full_path, new_path)\n"
      ],
      "metadata": {
        "id": "vMpgngfkrc0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms import functional as F\n",
        "from torchvision.datasets.utils import download_and_extract_archive, verify_str_arg\n",
        "\n",
        "from detectron2.structures import BoxMode\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import collections\n",
        "import os\n",
        "from xml.etree.ElementTree import Element as ET_Element\n",
        "\n",
        "try:\n",
        "    from defusedxml.ElementTree import parse as ET_parse\n",
        "except ImportError:\n",
        "    from xml.etree.ElementTree import parse as ET_parse\n",
        "from typing import Any, Dict\n",
        "\n",
        "def parse_voc_xml(node: ET_Element) -> Dict[str, Any]:\n",
        "    voc_dict: Dict[str, Any] = {}\n",
        "    children = list(node)\n",
        "    if children:\n",
        "        def_dic: Dict[str, Any] = collections.defaultdict(list)\n",
        "        for dc in map(parse_voc_xml, children):\n",
        "            for ind, v in dc.items():\n",
        "                def_dic[ind].append(v)\n",
        "        if node.tag == \"annotation\":\n",
        "            def_dic[\"object\"] = [def_dic[\"object\"]]\n",
        "        voc_dict = {\n",
        "            node.tag: {\n",
        "                ind: v[0] if len(v) == 1 else v for ind, v in def_dic.items()\n",
        "            }\n",
        "        }\n",
        "    if node.text:\n",
        "        text = node.text.strip()\n",
        "        if not children:\n",
        "            voc_dict[node.tag] = text\n",
        "    return voc_dict\n",
        "\n",
        "def get_fracture_dicts(\n",
        "    root: str,\n",
        "    image_set: str = \"train\",\n",
        "):\n",
        "    valid_image_sets = [\"train\", \"test\"]\n",
        "    image_set = verify_str_arg(image_set, \"image_set\", valid_image_sets)\n",
        "\n",
        "    url = \"https://figshare.com/ndownloader/files/41725659\"\n",
        "    filename = \"fracatlas.zip\"\n",
        "\n",
        "    # if download:\n",
        "    if not os.path.isdir(\"data/FracAtlas\"):\n",
        "        os.makedirs(\"data\", exist_ok=True)\n",
        "        download_and_extract_archive(\n",
        "            url,\n",
        "            os.path.dirname(root),\n",
        "            filename=filename,\n",
        "            remove_finished=True,\n",
        "        )\n",
        "        for subdir in [\"Fractured\", \"Non_fractured\"]:\n",
        "            dirpath = os.path.join(root, \"images\")\n",
        "            subdirpath = os.path.join(dirpath, subdir)\n",
        "            for f in os.listdir(subdirpath):\n",
        "                if not f.lower().endswith(\".jpg\"):\n",
        "                    continue\n",
        "                os.rename(os.path.join(subdirpath, f), os.path.join(dirpath, f))\n",
        "            os.rmdir(subdirpath)\n",
        "        print(os.listdir(\"data\"))\n",
        "    if not os.path.isdir(root):\n",
        "        raise RuntimeError(\n",
        "            \"Dataset not found or corrupted. You can use download=True to download it\"\n",
        "        )\n",
        "\n",
        "    image_dir = os.path.join(root, \"images\")\n",
        "    target_dir = os.path.join(root, \"Annotations\", \"PASCAL VOC\")\n",
        "    all_images = [os.path.splitext(x)[0] for x in os.listdir(image_dir)]\n",
        "\n",
        "    # remove images without a fracture because we need bounding boxes to train\n",
        "    all_images = [x for x in all_images if len(parse_voc_xml(ET_parse(os.path.join(target_dir, x + \".xml\")).getroot())[\"annotation\"][\"object\"]) != 0]\n",
        "\n",
        "    # 90% of images in train, and the last 10% in test\n",
        "    file_names = []\n",
        "    if image_set == \"train\":\n",
        "        file_names = all_images[: int(0.9 * len(all_images))]\n",
        "    else:\n",
        "        file_names = all_images[int(0.9 * len(all_images)) :]\n",
        "\n",
        "    images = [os.path.join(image_dir, x + \".jpg\") for x in file_names]\n",
        "    targets = [os.path.join(target_dir, x + \".xml\") for x in file_names]\n",
        "    assert len(images) == len(targets)\n",
        "\n",
        "    dataset_dicts = []\n",
        "    for index, image in enumerate(images):\n",
        "        img = Image.open(image).convert(\"RGB\")\n",
        "        img = F.to_tensor(img)\n",
        "        item = parse_voc_xml(ET_parse(targets[index]).getroot())\n",
        "\n",
        "        objects = [\n",
        "            {\n",
        "                \"bbox\": [\n",
        "                    int(obj[\"bndbox\"][\"xmin\"]),\n",
        "                    int(obj[\"bndbox\"][\"ymin\"]),\n",
        "                    int(obj[\"bndbox\"][\"xmax\"]),\n",
        "                    int(obj[\"bndbox\"][\"ymax\"]),\n",
        "                ],\n",
        "                \"bbox_mode\": BoxMode.XYXY_ABS,\n",
        "                \"category_id\": 0,\n",
        "            }\n",
        "            for obj in item[\"annotation\"][\"object\"]\n",
        "        ]\n",
        "\n",
        "        target = {}\n",
        "        target[\"file_name\"] = images[index]\n",
        "        target[\"image_id\"] = index\n",
        "        target[\"width\"] = int(item[\"annotation\"][\"size\"][\"width\"])\n",
        "        target[\"height\"] = int(item[\"annotation\"][\"size\"][\"height\"])\n",
        "        target[\"annotations\"] = objects\n",
        "        dataset_dicts.append(target)\n",
        "    return dataset_dicts\n",
        "\n",
        "for d in [\"train\", \"test\"]:\n",
        "    DatasetCatalog.register(\"fracture_\" + d, lambda d=d: get_fracture_dicts(\"data/FracAtlas\", d))\n",
        "    MetadataCatalog.get(\"fracture_\" + d).set(thing_classes=[\"fracture\"])\n",
        "fracture_metadata = MetadataCatalog.get(\"fracture_train\")\n"
      ],
      "metadata": {
        "id": "nFpZiIvyxQ7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KY2PT43DrHxg"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/mtroym/EfficientDet.detectron2\n",
        "%cd \"EfficientDet.detectron2\"\n",
        "!DETECTRON2_DATASETS=../data/ python3 train.py --config-file configs/Base-EfficientDet.yaml --opts DATASETS.TRAIN fracture_train DATASETS.TEST fracture_test"
      ]
    }
  ]
}