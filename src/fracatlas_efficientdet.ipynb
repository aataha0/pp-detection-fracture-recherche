{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/botatooo/pp-detection-fracture-recherche/blob/dev/src/fracatlas_efficientdet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaehsA6usXtQ",
        "outputId": "bfcf7b29-85e6-4330-e5b0-4374e8ad8b52"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/facebookresearch/detectron2.git\n",
            "  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-req-build-g8zqne31\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-g8zqne31\n",
            "  Resolved https://github.com/facebookresearch/detectron2.git to commit 864913f0e57e87a75c8cc0c7d79ecbd774fc669b\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (9.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (3.7.1)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.0.7)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.4.0)\n",
            "Requirement already satisfied: yacs>=0.1.8 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (0.1.8)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (0.9.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.2.1)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (4.66.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.15.1)\n",
            "Requirement already satisfied: fvcore<0.1.6,>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (0.1.5.post20221221)\n",
            "Requirement already satisfied: iopath<0.1.10,>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (0.1.9)\n",
            "Requirement already satisfied: omegaconf<2.4,>=2.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.3.0)\n",
            "Requirement already satisfied: hydra-core>=1.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (1.3.2)\n",
            "Requirement already satisfied: black in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (23.12.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (23.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (1.23.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (6.0.1)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from hydra-core>=1.1->detectron2==0.6) (4.9.3)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from iopath<0.1.10,>=0.1.7->detectron2==0.6) (2.8.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (4.47.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (2.8.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (8.1.7)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (1.0.0)\n",
            "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (0.12.1)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (4.1.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (2.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (4.5.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.60.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.5.1)\n",
            "Requirement already satisfied: protobuf<4.24,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (2.31.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard->detectron2==0.6) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.6) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->detectron2==0.6) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, detectron2\n",
        "\n",
        "!nvcc --version\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
        "print(\"detectron2:\", detectron2.__version__)\n"
      ],
      "metadata": {
        "id": "Ynemyv6St0De",
        "outputId": "800da48a-fff5-4a0a-eabc-b95b8ed75af7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n",
            "torch:  2.1 ; cuda:  cu121\n",
            "detectron2: 0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from torchvision.datasets.utils import download_and_extract_archive\n",
        "\n",
        "root = \"dataset/\"\n",
        "url = \"https://figshare.com/ndownloader/files/41725659\"\n",
        "filename = \"fracatlas.zip\"\n",
        "\n",
        "# if download:\n",
        "if not os.path.isdir(os.path.join(root, \"FracAtlas\")):\n",
        "    os.makedirs(root, exist_ok=True)\n",
        "    download_and_extract_archive(\n",
        "        url,\n",
        "        os.path.dirname(root),\n",
        "        filename=filename,\n",
        "        remove_finished=True,\n",
        "    )\n",
        "if not os.path.isdir(root):\n",
        "    raise RuntimeError(\n",
        "        \"Dataset not found or corrupted. You can use download=True to download it\"\n",
        "    )\n",
        "\n",
        "with open(\"dataset/FracAtlas/Annotations/COCO JSON/COCO_fracture_masks.json\") as f:\n",
        "  fracture_masks_data = json.load(f)\n",
        "\n",
        "fractured_images = [i[\"file_name\"] for i in fracture_masks_data[\"images\"]]\n",
        "fractured_image_count = len(fractured_images)\n",
        "\n",
        "training_images = fractured_images[: int(0.9 * fractured_image_count)]\n",
        "testing_images = fractured_images[int(0.9 * fractured_image_count) :]\n",
        "\n",
        "\n",
        "os.mkdir(\"data\")\n",
        "os.mkdir(\"data/fracatlas\")\n",
        "\n",
        "\n",
        "os.mkdir(\"data/fracatlas/images\")\n",
        "\n",
        "os.mkdir(\"data/fracatlas/images/train\")\n",
        "for i in training_images:\n",
        "  full_path = os.path.abspath(os.path.join(\"dataset/FracAtlas/images/Fractured\", i))\n",
        "  new_path = os.path.abspath(os.path.join(\"data/fracatlas/images/train\", i))\n",
        "  os.rename(full_path, new_path)\n",
        "\n",
        "os.mkdir(\"data/fracatlas/images/val\")\n",
        "for i in testing_images:\n",
        "  full_path = os.path.abspath(os.path.join(\"dataset/FracAtlas/images/Fractured\", i))\n",
        "  new_path = os.path.abspath(os.path.join(\"data/fracatlas/images/val\", i))\n",
        "  os.rename(full_path, new_path)\n",
        "\n",
        "\n",
        "os.mkdir(\"data/fracatlas/labels\")\n",
        "\n",
        "os.mkdir(\"data/fracatlas/labels/train\")\n",
        "for i in training_images:\n",
        "  i = i.replace(\".jpg\", \".txt\")\n",
        "  full_path = os.path.abspath(os.path.join(\"dataset/FracAtlas/Annotations/YOLO\", i))\n",
        "  new_path = os.path.abspath(os.path.join(\"data/fracatlas/labels/train\", i))\n",
        "  os.rename(full_path, new_path)\n",
        "\n",
        "os.mkdir(\"data/fracatlas/labels/val\")\n",
        "for i in testing_images:\n",
        "  i = i.replace(\".jpg\", \".txt\")\n",
        "  full_path = os.path.abspath(os.path.join(\"dataset/FracAtlas/Annotations/YOLO\", i))\n",
        "  new_path = os.path.abspath(os.path.join(\"data/fracatlas/labels/val\", i))\n",
        "  os.rename(full_path, new_path)\n"
      ],
      "metadata": {
        "id": "vMpgngfkrc0s",
        "outputId": "da704598-358e-4226-ce58-0f2f9aa3bed9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileExistsError",
          "evalue": "[Errno 17] File exists: 'data'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-630050ddc334>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/fracatlas\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: 'data'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms import functional as F\n",
        "from torchvision.datasets.utils import download_and_extract_archive, verify_str_arg\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "\n",
        "from detectron2.structures import BoxMode\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import collections\n",
        "import os\n",
        "from xml.etree.ElementTree import Element as ET_Element\n",
        "\n",
        "try:\n",
        "    from defusedxml.ElementTree import parse as ET_parse\n",
        "except ImportError:\n",
        "    from xml.etree.ElementTree import parse as ET_parse\n",
        "from typing import Any, Dict\n",
        "\n",
        "def parse_voc_xml(node: ET_Element) -> Dict[str, Any]:\n",
        "    voc_dict: Dict[str, Any] = {}\n",
        "    children = list(node)\n",
        "    if children:\n",
        "        def_dic: Dict[str, Any] = collections.defaultdict(list)\n",
        "        for dc in map(parse_voc_xml, children):\n",
        "            for ind, v in dc.items():\n",
        "                def_dic[ind].append(v)\n",
        "        if node.tag == \"annotation\":\n",
        "            def_dic[\"object\"] = [def_dic[\"object\"]]\n",
        "        voc_dict = {\n",
        "            node.tag: {\n",
        "                ind: v[0] if len(v) == 1 else v for ind, v in def_dic.items()\n",
        "            }\n",
        "        }\n",
        "    if node.text:\n",
        "        text = node.text.strip()\n",
        "        if not children:\n",
        "            voc_dict[node.tag] = text\n",
        "    return voc_dict\n",
        "\n",
        "def get_fracture_dicts(\n",
        "    root: str,\n",
        "    image_set: str = \"train\",\n",
        "):\n",
        "    valid_image_sets = [\"train\", \"test\"]\n",
        "    image_set = verify_str_arg(image_set, \"image_set\", valid_image_sets)\n",
        "\n",
        "    url = \"https://figshare.com/ndownloader/files/41725659\"\n",
        "    filename = \"fracatlas.zip\"\n",
        "\n",
        "    # if download:\n",
        "    if not os.path.isdir(\"data/FracAtlas\"):\n",
        "        os.makedirs(\"data\", exist_ok=True)\n",
        "        download_and_extract_archive(\n",
        "            url,\n",
        "            os.path.dirname(root),\n",
        "            filename=filename,\n",
        "            remove_finished=True,\n",
        "        )\n",
        "        for subdir in [\"Fractured\", \"Non_fractured\"]:\n",
        "            dirpath = os.path.join(root, \"images\")\n",
        "            subdirpath = os.path.join(dirpath, subdir)\n",
        "            for f in os.listdir(subdirpath):\n",
        "                if not f.lower().endswith(\".jpg\"):\n",
        "                    continue\n",
        "                os.rename(os.path.join(subdirpath, f), os.path.join(dirpath, f))\n",
        "            os.rmdir(subdirpath)\n",
        "        print(os.listdir(\"data\"))\n",
        "    if not os.path.isdir(root):\n",
        "        raise RuntimeError(\n",
        "            \"Dataset not found or corrupted. You can use download=True to download it\"\n",
        "        )\n",
        "\n",
        "    image_dir = os.path.join(root, \"images\")\n",
        "    target_dir = os.path.join(root, \"Annotations\", \"PASCAL VOC\")\n",
        "    all_images = [os.path.splitext(x)[0] for x in os.listdir(image_dir)]\n",
        "\n",
        "    # remove images without a fracture because we need bounding boxes to train\n",
        "    all_images = [x for x in all_images if len(parse_voc_xml(ET_parse(os.path.join(target_dir, x + \".xml\")).getroot())[\"annotation\"][\"object\"]) != 0]\n",
        "\n",
        "    # 90% of images in train, and the last 10% in test\n",
        "    file_names = []\n",
        "    if image_set == \"train\":\n",
        "        file_names = all_images[: int(0.9 * len(all_images))]\n",
        "    else:\n",
        "        file_names = all_images[int(0.9 * len(all_images)) :]\n",
        "\n",
        "    images = [os.path.join(image_dir, x + \".jpg\") for x in file_names]\n",
        "    targets = [os.path.join(target_dir, x + \".xml\") for x in file_names]\n",
        "    assert len(images) == len(targets)\n",
        "\n",
        "    dataset_dicts = []\n",
        "    for index, image in enumerate(images):\n",
        "        img = Image.open(image).convert(\"RGB\")\n",
        "        img = F.to_tensor(img)\n",
        "        item = parse_voc_xml(ET_parse(targets[index]).getroot())\n",
        "\n",
        "        objects = [\n",
        "            {\n",
        "                \"bbox\": [\n",
        "                    int(obj[\"bndbox\"][\"xmin\"]),\n",
        "                    int(obj[\"bndbox\"][\"ymin\"]),\n",
        "                    int(obj[\"bndbox\"][\"xmax\"]),\n",
        "                    int(obj[\"bndbox\"][\"ymax\"]),\n",
        "                ],\n",
        "                \"bbox_mode\": BoxMode.XYXY_ABS,\n",
        "                \"category_id\": 0,\n",
        "            }\n",
        "            for obj in item[\"annotation\"][\"object\"]\n",
        "        ]\n",
        "\n",
        "        target = {}\n",
        "        target[\"file_name\"] = images[index]\n",
        "        target[\"image_id\"] = index\n",
        "        target[\"width\"] = int(item[\"annotation\"][\"size\"][\"width\"])\n",
        "        target[\"height\"] = int(item[\"annotation\"][\"size\"][\"height\"])\n",
        "        target[\"annotations\"] = objects\n",
        "        dataset_dicts.append(target)\n",
        "    return dataset_dicts\n"
      ],
      "metadata": {
        "id": "nFpZiIvyxQ7k"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KY2PT43DrHxg"
      },
      "outputs": [],
      "source": [
        "!DETECTRON2_DATASETS=../data/ python3 train.py --config-file configs/Base-EfficientDet.yaml \"DATASETS.TRAIN\" \"('fracture_train',)\" \"DATASETS.TEST\" \"('fracture_test',)\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/mtroym/EfficientDet.detectron2\n",
        "%cd \"EfficientDet.detectron2\"\n",
        "\n",
        "import logging\n",
        "import os\n",
        "from collections import OrderedDict\n",
        "\n",
        "import detectron2.utils.comm as comm\n",
        "import torch\n",
        "from detectron2.checkpoint import DetectionCheckpointer, PeriodicCheckpointer\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.data import (\n",
        "    MetadataCatalog,\n",
        "    build_detection_test_loader,\n",
        "    build_detection_train_loader,\n",
        ")\n",
        "from detectron2.engine import default_argument_parser, default_setup, launch\n",
        "from detectron2.evaluation import (\n",
        "\n",
        "    # CityscapesEvaluator,\n",
        "    COCOEvaluator,\n",
        "    COCOPanopticEvaluator,\n",
        "    DatasetEvaluators,\n",
        "    LVISEvaluator,\n",
        "    PascalVOCDetectionEvaluator,\n",
        "    SemSegEvaluator,\n",
        "    inference_on_dataset,\n",
        "    print_csv_format,\n",
        ")\n",
        "from detectron2.modeling import build_model\n",
        "from detectron2.solver import build_lr_scheduler, build_optimizer\n",
        "from detectron2.utils.events import (\n",
        "    CommonMetricPrinter,\n",
        "    EventStorage,\n",
        "    JSONWriter,\n",
        "    TensorboardXWriter,\n",
        ")\n",
        "from torch.nn.parallel import DistributedDataParallel\n",
        "\n",
        "from src.config import add_efficientdet_config\n",
        "from src.data import register_all_df2\n",
        "from src.modeling.efficientdet_heads import EfficientDetHead\n",
        "from src.data import DetDatasetMapper\n",
        "\n",
        "logger = logging.getLogger(\"detectron2\")\n",
        "\n",
        "\n",
        "# register_all_df2(\"/mnt/cephfs_new_wj/lab_ad_idea/maoyiming/data\")\n",
        "\n",
        "\n",
        "def get_evaluator(cfg, dataset_name, output_folder=None):\n",
        "    \"\"\"\n",
        "    Create evaluator(s) for a given dataset.\n",
        "    This uses the special metadata \"evaluator_type\" associated with each builtin dataset.\n",
        "    For your own dataset, you can simply create an evaluator manually in your\n",
        "    script and do not have to worry about the hacky if-else logic here.\n",
        "    \"\"\"\n",
        "    if output_folder is None:\n",
        "        output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
        "    evaluator_list = []\n",
        "    evaluator_type = MetadataCatalog.get(dataset_name).evaluator_type\n",
        "    if evaluator_type in [\"sem_seg\", \"coco_panoptic_seg\"]:\n",
        "        evaluator_list.append(\n",
        "            SemSegEvaluator(\n",
        "                dataset_name,\n",
        "                distributed=True,\n",
        "                num_classes=cfg.MODEL.SEM_SEG_HEAD.NUM_CLASSES,\n",
        "                ignore_label=cfg.MODEL.SEM_SEG_HEAD.IGNORE_VALUE,\n",
        "                output_dir=output_folder,\n",
        "            )\n",
        "        )\n",
        "    if evaluator_type in [\"coco\", \"coco_panoptic_seg\"]:\n",
        "        evaluator_list.append(COCOEvaluator(dataset_name, cfg, True, output_folder))\n",
        "    if evaluator_type == \"coco_panoptic_seg\":\n",
        "        evaluator_list.append(COCOPanopticEvaluator(dataset_name, output_folder))\n",
        "    # if evaluator_type == \"cityscapes\":\n",
        "    # assert (\n",
        "    #         torch.cuda.device_count() >= comm.get_rank()\n",
        "    # ), \"CityscapesEvaluator currently do not work with multiple machines.\"\n",
        "    # return CityscapesEvaluator(dataset_name)\n",
        "    if evaluator_type == \"pascal_voc\":\n",
        "        return PascalVOCDetectionEvaluator(dataset_name)\n",
        "    if evaluator_type == \"lvis\":\n",
        "        return LVISEvaluator(dataset_name, cfg, True, output_folder)\n",
        "    if len(evaluator_list) == 0:\n",
        "        raise NotImplementedError(\n",
        "            \"no Evaluator for the dataset {} with the type {}\".format(dataset_name, evaluator_type)\n",
        "        )\n",
        "    if len(evaluator_list) == 1:\n",
        "        return evaluator_list[0]\n",
        "    return DatasetEvaluators(evaluator_list)\n",
        "\n",
        "\n",
        "def do_test(cfg, model):\n",
        "    results = OrderedDict()\n",
        "    for dataset_name in cfg.DATASETS.TEST:\n",
        "        data_loader = build_detection_test_loader(cfg, dataset_name,\n",
        "                                                  mapper=DetDatasetMapper(cfg, is_train=False))\n",
        "        evaluator = get_evaluator(\n",
        "            cfg, dataset_name, os.path.join(cfg.OUTPUT_DIR, \"inference\", dataset_name)\n",
        "        )\n",
        "        results_i = inference_on_dataset(model, data_loader, evaluator)\n",
        "        results[dataset_name] = results_i\n",
        "        if comm.is_main_process():\n",
        "            logger.info(\"Evaluation results for {} in csv format:\".format(dataset_name))\n",
        "            print_csv_format(results_i)\n",
        "    if len(results) == 1:\n",
        "        results = list(results.values())[0]\n",
        "    return results\n",
        "\n",
        "\n",
        "def do_train(cfg, model, resume=False):\n",
        "    model.train()\n",
        "    optimizer = build_optimizer(cfg, model)\n",
        "    scheduler = build_lr_scheduler(cfg, optimizer)\n",
        "\n",
        "    checkpointer = DetectionCheckpointer(\n",
        "        model, cfg.OUTPUT_DIR, optimizer=optimizer, scheduler=scheduler\n",
        "    )\n",
        "    start_iter = (\n",
        "            checkpointer.resume_or_load(cfg.MODEL.WEIGHTS, resume=resume).get(\"iteration\", -1) + 1\n",
        "    )\n",
        "    max_iter = cfg.SOLVER.MAX_ITER\n",
        "\n",
        "    periodic_checkpointer = PeriodicCheckpointer(\n",
        "        checkpointer, cfg.SOLVER.CHECKPOINT_PERIOD, max_iter=max_iter\n",
        "    )\n",
        "\n",
        "    writers = (\n",
        "        [\n",
        "            CommonMetricPrinter(max_iter),\n",
        "            JSONWriter(os.path.join(cfg.OUTPUT_DIR, \"metrics.json\")),\n",
        "            TensorboardXWriter(cfg.OUTPUT_DIR),\n",
        "        ]\n",
        "        if comm.is_main_process()\n",
        "        else []\n",
        "    )\n",
        "\n",
        "    # compared to \"train_net.py\", we do not support accurate timing and\n",
        "    # precise BN here, because they are not trivial to implement\n",
        "    dataset_mapper = DetDatasetMapper(cfg, is_train=True)\n",
        "    data_loader = build_detection_train_loader(cfg, mapper=dataset_mapper)\n",
        "\n",
        "    logger.info(\"builded detection train loader by DatasetMapper\")\n",
        "    logger.info(\"Starting training from iteration {}\".format(start_iter))\n",
        "    with EventStorage(start_iter) as storage:\n",
        "        for data, iteration in zip(data_loader, range(start_iter, max_iter)):\n",
        "            iteration = iteration + 1\n",
        "            storage.step()\n",
        "\n",
        "            loss_dict = model(data)\n",
        "            losses = sum(loss_dict.values())\n",
        "            assert torch.isfinite(losses).all(), loss_dict\n",
        "\n",
        "            loss_dict_reduced = {k: v.item() for k, v in comm.reduce_dict(loss_dict).items()}\n",
        "            losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
        "            if comm.is_main_process():\n",
        "                storage.put_scalars(total_loss=losses_reduced, **loss_dict_reduced)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            losses.backward()\n",
        "            optimizer.step()\n",
        "            storage.put_scalar(\"lr\", optimizer.param_groups[0][\"lr\"], smoothing_hint=False)\n",
        "            scheduler.step(None)\n",
        "\n",
        "            if (\n",
        "                    cfg.TEST.EVAL_PERIOD > 0\n",
        "                    and iteration % cfg.TEST.EVAL_PERIOD == 0\n",
        "                    and iteration != max_iter\n",
        "            ):\n",
        "                do_test(cfg, model)\n",
        "                # Compared to \"train_net.py\", the test results are not dumped to EventStorage\n",
        "                comm.synchronize()\n",
        "\n",
        "            if iteration - start_iter > 5 and (iteration % 20 == 0 or iteration == max_iter):\n",
        "                for writer in writers:\n",
        "                    writer.write()\n",
        "            periodic_checkpointer.step(iteration)\n",
        "\n",
        "\n",
        "def setup():\n",
        "    \"\"\"\n",
        "    Create configs and perform basic setups.\n",
        "    \"\"\"\n",
        "    for d in [\"train\", \"test\"]:\n",
        "        DatasetCatalog.register(\"fracture_\" + d, lambda: get_fracture_dicts(\"data/FracAtlas\", d))\n",
        "        MetadataCatalog.get(\"fracture_\" + d).set(thing_classes=[\"fracture\"])\n",
        "    fracture_metadata = MetadataCatalog.get(\"fracture_train\")\n",
        "\n",
        "    cfg = get_cfg()\n",
        "    cfg = add_efficientdet_config(cfg)\n",
        "    # cfg.merge_from_file(args.config_file)\n",
        "    cfg.DATASETS.TRAIN = (\"fracture_train\",)\n",
        "    cfg.DATASETS.TEST = (\"fracture_test\",)\n",
        "    cfg.freeze()\n",
        "    # default_setup(\n",
        "    #     cfg # , args\n",
        "    # )  # if you don't like any of the default setup, write your own setup code\n",
        "    return cfg\n",
        "\n",
        "\n",
        "def main():\n",
        "    cfg = setup()\n",
        "\n",
        "    model = build_model(cfg)\n",
        "    # logger.info(\"Model:\\n{}\".format(model))\n",
        "    # if args.eval_only:\n",
        "    #     DetectionCheckpointer(model, save_dir=cfg.OUTPUT_DIR).resume_or_load(\n",
        "    #         cfg.MODEL.WEIGHTS, resume=args.resume\n",
        "    #     )\n",
        "    #     return do_test(cfg, model)\n",
        "\n",
        "    distributed = comm.get_world_size() > 1\n",
        "    if distributed:\n",
        "        model = DistributedDataParallel(\n",
        "            model, device_ids=[comm.get_local_rank()], broadcast_buffers=False,\n",
        "            find_unused_parameters=True\n",
        "        )\n",
        "    do_train(cfg, model)\n",
        "    return do_test(cfg, model)\n",
        "\n",
        "\n",
        "launch(\n",
        "    main,\n",
        "    1,\n",
        "    num_machines=1,\n",
        "    machine_rank=0,\n",
        "    dist_url=\"tcp://127.0.0.1:{}\".format(2**15 + 2**14 + hash(os.getuid()) % 2**14),\n",
        "    # args=(args,),\n",
        ")"
      ],
      "metadata": {
        "id": "Bl2LdhRhnPc0",
        "outputId": "9159ad3d-d5b0-4ad7-d734-89e951c4ba53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'EfficientDet.detectron2' already exists and is not an empty directory.\n",
            "/content/EfficientDet.detectron2\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'output/metrics.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-af6b3b9415c4>\u001b[0m in \u001b[0;36m<cell line: 223>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m launch(\n\u001b[0m\u001b[1;32m    224\u001b[0m     \u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/detectron2/engine/launch.py\u001b[0m in \u001b[0;36mlaunch\u001b[0;34m(main_func, num_gpus_per_machine, num_machines, machine_rank, dist_url, args, timeout)\u001b[0m\n\u001b[1;32m     82\u001b[0m         )\n\u001b[1;32m     83\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mmain_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-af6b3b9415c4>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0mfind_unused_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         )\n\u001b[0;32m--> 219\u001b[0;31m     \u001b[0mdo_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdo_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-af6b3b9415c4>\u001b[0m in \u001b[0;36mdo_train\u001b[0;34m(cfg, model, resume)\u001b[0m\n\u001b[1;32m    130\u001b[0m         [\n\u001b[1;32m    131\u001b[0m             \u001b[0mCommonMetricPrinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0mJSONWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOUTPUT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"metrics.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m             \u001b[0mTensorboardXWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOUTPUT_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         ]\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/detectron2/utils/events.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, json_file, window_size)\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0;31m`\u001b[0m\u001b[0msmoothing_hint\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \"\"\"\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPathManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"a\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_window_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_write\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/iopath/common/file_io.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, buffering, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         \"\"\"\n\u001b[1;32m   1011\u001b[0m         \u001b[0mhandler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_path_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1012\u001b[0;31m         \u001b[0mbret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0mkvs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_open_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/iopath/common/file_io.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, path, mode, buffering, encoding, errors, newline, closefd, opener, **kwargs)\u001b[0m\n\u001b[1;32m    602\u001b[0m         \"\"\"\n\u001b[1;32m    603\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m         return open(  # type: ignore\n\u001b[0m\u001b[1;32m    605\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_path_with_cwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m             \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'output/metrics.json'"
          ]
        }
      ]
    }
  ]
}