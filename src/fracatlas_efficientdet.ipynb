{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/botatooo/pp-detection-fracture-recherche/blob/dev/src/fracatlas_efficientdet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaehsA6usXtQ",
        "outputId": "784833a8-b9d3-4dbc-be34-9b19c3d93424"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/facebookresearch/detectron2.git\n",
            "  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-req-build-5gxpb72s\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-5gxpb72s\n",
            "  Resolved https://github.com/facebookresearch/detectron2.git to commit 864913f0e57e87a75c8cc0c7d79ecbd774fc669b\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (9.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (3.7.1)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.0.7)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.4.0)\n",
            "Collecting yacs>=0.1.8 (from detectron2==0.6)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (0.9.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.2.1)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (4.66.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.15.1)\n",
            "Collecting fvcore<0.1.6,>=0.1.5 (from detectron2==0.6)\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting iopath<0.1.10,>=0.1.7 (from detectron2==0.6)\n",
            "  Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
            "Collecting omegaconf<2.4,>=2.1 (from detectron2==0.6)\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hydra-core>=1.1 (from detectron2==0.6)\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting black (from detectron2==0.6)\n",
            "  Downloading black-23.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (23.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (1.23.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (6.0.1)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from hydra-core>=1.1->detectron2==0.6)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting portalocker (from iopath<0.1.10,>=0.1.7->detectron2==0.6)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (4.47.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (2.8.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (8.1.7)\n",
            "Collecting mypy-extensions>=0.4.3 (from black->detectron2==0.6)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting pathspec>=0.9.0 (from black->detectron2==0.6)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (4.1.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (2.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (4.5.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.60.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.5.1)\n",
            "Requirement already satisfied: protobuf<4.24,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (2.31.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard->detectron2==0.6) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.6) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->detectron2==0.6) (3.2.2)\n",
            "Building wheels for collected packages: detectron2, fvcore, antlr4-python3-runtime\n",
            "  Building wheel for detectron2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for detectron2: filename=detectron2-0.6-cp310-cp310-linux_x86_64.whl size=6119643 sha256=2469ddaae1eb85c4b32151edeb67235602a521f31f0cb898e965ec4af5a54c7e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-wdsv5r57/wheels/47/e5/15/94c80df2ba85500c5d76599cc307c0a7079d0e221bb6fc4375\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61400 sha256=9db2eadd2e2b845222f37c67aebcc01a8d6a43b208936f0288c979228a15c961\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=a51720b61e646db7977a2687f769693c344c01129e47ba533b37fade25b5d4d4\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "Successfully built detectron2 fvcore antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, yacs, portalocker, pathspec, omegaconf, mypy-extensions, iopath, hydra-core, black, fvcore, detectron2\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 black-23.12.1 detectron2-0.6 fvcore-0.1.5.post20221221 hydra-core-1.3.2 iopath-0.1.9 mypy-extensions-1.0.0 omegaconf-2.3.0 pathspec-0.12.1 portalocker-2.8.2 yacs-0.1.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, detectron2\n",
        "\n",
        "!nvcc --version\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
        "print(\"detectron2:\", detectron2.__version__)\n"
      ],
      "metadata": {
        "id": "Ynemyv6St0De",
        "outputId": "fa3adc4b-4bbf-46ff-b3b3-86de93840fba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n",
            "torch:  2.1 ; cuda:  cu121\n",
            "detectron2: 0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from torchvision.datasets.utils import download_and_extract_archive\n",
        "\n",
        "root = \"dataset/\"\n",
        "url = \"https://figshare.com/ndownloader/files/41725659\"\n",
        "filename = \"fracatlas.zip\"\n",
        "\n",
        "# if download:\n",
        "if not os.path.isdir(os.path.join(root, \"FracAtlas\")):\n",
        "    os.makedirs(root, exist_ok=True)\n",
        "    download_and_extract_archive(\n",
        "        url,\n",
        "        os.path.dirname(root),\n",
        "        filename=filename,\n",
        "        remove_finished=True,\n",
        "    )\n",
        "if not os.path.isdir(root):\n",
        "    raise RuntimeError(\n",
        "        \"Dataset not found or corrupted. You can use download=True to download it\"\n",
        "    )\n",
        "\n",
        "with open(\"dataset/FracAtlas/Annotations/COCO JSON/COCO_fracture_masks.json\") as f:\n",
        "  fracture_masks_data = json.load(f)\n",
        "\n",
        "fractured_images = [i[\"file_name\"] for i in fracture_masks_data[\"images\"]]\n",
        "fractured_image_count = len(fractured_images)\n",
        "\n",
        "training_images = fractured_images[: int(0.9 * fractured_image_count)]\n",
        "testing_images = fractured_images[int(0.9 * fractured_image_count) :]\n",
        "\n",
        "\n",
        "os.mkdir(\"data\")\n",
        "os.mkdir(\"data/fracatlas\")\n",
        "\n",
        "\n",
        "os.mkdir(\"data/fracatlas/images\")\n",
        "\n",
        "os.mkdir(\"data/fracatlas/images/train\")\n",
        "for i in training_images:\n",
        "  full_path = os.path.abspath(os.path.join(\"dataset/FracAtlas/images/Fractured\", i))\n",
        "  new_path = os.path.abspath(os.path.join(\"data/fracatlas/images/train\", i))\n",
        "  os.rename(full_path, new_path)\n",
        "\n",
        "os.mkdir(\"data/fracatlas/images/val\")\n",
        "for i in testing_images:\n",
        "  full_path = os.path.abspath(os.path.join(\"dataset/FracAtlas/images/Fractured\", i))\n",
        "  new_path = os.path.abspath(os.path.join(\"data/fracatlas/images/val\", i))\n",
        "  os.rename(full_path, new_path)\n",
        "\n",
        "\n",
        "os.mkdir(\"data/fracatlas/labels\")\n",
        "\n",
        "os.mkdir(\"data/fracatlas/labels/train\")\n",
        "for i in training_images:\n",
        "  i = i.replace(\".jpg\", \".txt\")\n",
        "  full_path = os.path.abspath(os.path.join(\"dataset/FracAtlas/Annotations/YOLO\", i))\n",
        "  new_path = os.path.abspath(os.path.join(\"data/fracatlas/labels/train\", i))\n",
        "  os.rename(full_path, new_path)\n",
        "\n",
        "os.mkdir(\"data/fracatlas/labels/val\")\n",
        "for i in testing_images:\n",
        "  i = i.replace(\".jpg\", \".txt\")\n",
        "  full_path = os.path.abspath(os.path.join(\"dataset/FracAtlas/Annotations/YOLO\", i))\n",
        "  new_path = os.path.abspath(os.path.join(\"data/fracatlas/labels/val\", i))\n",
        "  os.rename(full_path, new_path)\n"
      ],
      "metadata": {
        "id": "vMpgngfkrc0s",
        "outputId": "3e4565f3-5e64-4ef2-e19a-c3e46dad479e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/41725659/FracAtlas.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIYCQYOYV5JSSROOA/20240110/eu-west-1/s3/aws4_request&X-Amz-Date=20240110T043444Z&X-Amz-Expires=10&X-Amz-SignedHeaders=host&X-Amz-Signature=f2c08430a62d78cd5d9e426d6b608cb3b58f1a6a308302df7fde21b27d5b22cf to dataset/fracatlas.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 338412751/338412751 [00:03<00:00, 100856154.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset/fracatlas.zip to dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms import functional as F\n",
        "from torchvision.datasets.utils import download_and_extract_archive, verify_str_arg\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "\n",
        "from detectron2.structures import BoxMode\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import collections\n",
        "import os\n",
        "from xml.etree.ElementTree import Element as ET_Element\n",
        "\n",
        "try:\n",
        "    from defusedxml.ElementTree import parse as ET_parse\n",
        "except ImportError:\n",
        "    from xml.etree.ElementTree import parse as ET_parse\n",
        "from typing import Any, Dict\n",
        "\n",
        "def parse_voc_xml(node: ET_Element) -> Dict[str, Any]:\n",
        "    voc_dict: Dict[str, Any] = {}\n",
        "    children = list(node)\n",
        "    if children:\n",
        "        def_dic: Dict[str, Any] = collections.defaultdict(list)\n",
        "        for dc in map(parse_voc_xml, children):\n",
        "            for ind, v in dc.items():\n",
        "                def_dic[ind].append(v)\n",
        "        if node.tag == \"annotation\":\n",
        "            def_dic[\"object\"] = [def_dic[\"object\"]]\n",
        "        voc_dict = {\n",
        "            node.tag: {\n",
        "                ind: v[0] if len(v) == 1 else v for ind, v in def_dic.items()\n",
        "            }\n",
        "        }\n",
        "    if node.text:\n",
        "        text = node.text.strip()\n",
        "        if not children:\n",
        "            voc_dict[node.tag] = text\n",
        "    return voc_dict\n",
        "\n",
        "def get_fracture_dicts(\n",
        "    root: str,\n",
        "    image_set: str = \"train\",\n",
        "):\n",
        "    valid_image_sets = [\"train\", \"test\"]\n",
        "    image_set = verify_str_arg(image_set, \"image_set\", valid_image_sets)\n",
        "\n",
        "    url = \"https://figshare.com/ndownloader/files/41725659\"\n",
        "    filename = \"fracatlas.zip\"\n",
        "\n",
        "    # if download:\n",
        "    if not os.path.isdir(\"data/FracAtlas\"):\n",
        "        os.makedirs(\"data\", exist_ok=True)\n",
        "        download_and_extract_archive(\n",
        "            url,\n",
        "            os.path.dirname(root),\n",
        "            filename=filename,\n",
        "            remove_finished=True,\n",
        "        )\n",
        "        for subdir in [\"Fractured\", \"Non_fractured\"]:\n",
        "            dirpath = os.path.join(root, \"images\")\n",
        "            subdirpath = os.path.join(dirpath, subdir)\n",
        "            for f in os.listdir(subdirpath):\n",
        "                if not f.lower().endswith(\".jpg\"):\n",
        "                    continue\n",
        "                os.rename(os.path.join(subdirpath, f), os.path.join(dirpath, f))\n",
        "            os.rmdir(subdirpath)\n",
        "        print(os.listdir(\"data\"))\n",
        "    if not os.path.isdir(root):\n",
        "        raise RuntimeError(\n",
        "            \"Dataset not found or corrupted. You can use download=True to download it\"\n",
        "        )\n",
        "\n",
        "    image_dir = os.path.join(root, \"images\")\n",
        "    target_dir = os.path.join(root, \"Annotations\", \"PASCAL VOC\")\n",
        "    all_images = [os.path.splitext(x)[0] for x in os.listdir(image_dir)]\n",
        "\n",
        "    # remove images without a fracture because we need bounding boxes to train\n",
        "    all_images = [x for x in all_images if len(parse_voc_xml(ET_parse(os.path.join(target_dir, x + \".xml\")).getroot())[\"annotation\"][\"object\"]) != 0]\n",
        "\n",
        "    # 90% of images in train, and the last 10% in test\n",
        "    file_names = []\n",
        "    if image_set == \"train\":\n",
        "        file_names = all_images[: int(0.9 * len(all_images))]\n",
        "    else:\n",
        "        file_names = all_images[int(0.9 * len(all_images)) :]\n",
        "\n",
        "    images = [os.path.join(image_dir, x + \".jpg\") for x in file_names]\n",
        "    targets = [os.path.join(target_dir, x + \".xml\") for x in file_names]\n",
        "    assert len(images) == len(targets)\n",
        "\n",
        "    dataset_dicts = []\n",
        "    for index, image in enumerate(images):\n",
        "        img = Image.open(image).convert(\"RGB\")\n",
        "        img = F.to_tensor(img)\n",
        "        item = parse_voc_xml(ET_parse(targets[index]).getroot())\n",
        "\n",
        "        objects = [\n",
        "            {\n",
        "                \"bbox\": [\n",
        "                    int(obj[\"bndbox\"][\"xmin\"]),\n",
        "                    int(obj[\"bndbox\"][\"ymin\"]),\n",
        "                    int(obj[\"bndbox\"][\"xmax\"]),\n",
        "                    int(obj[\"bndbox\"][\"ymax\"]),\n",
        "                ],\n",
        "                \"bbox_mode\": BoxMode.XYXY_ABS,\n",
        "                \"category_id\": 0,\n",
        "            }\n",
        "            for obj in item[\"annotation\"][\"object\"]\n",
        "        ]\n",
        "\n",
        "        target = {}\n",
        "        target[\"file_name\"] = images[index]\n",
        "        target[\"image_id\"] = index\n",
        "        target[\"width\"] = int(item[\"annotation\"][\"size\"][\"width\"])\n",
        "        target[\"height\"] = int(item[\"annotation\"][\"size\"][\"height\"])\n",
        "        target[\"annotations\"] = objects\n",
        "        dataset_dicts.append(target)\n",
        "    return dataset_dicts\n"
      ],
      "metadata": {
        "id": "nFpZiIvyxQ7k"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KY2PT43DrHxg",
        "outputId": "a27a08bd-f720-4de4-f370-6d5fb18f27ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/train.py': [Errno 2] No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!DETECTRON2_DATASETS=../data/ python3 train.py --config-file configs/Base-EfficientDet.yaml \"DATASETS.TRAIN\" \"('fracture_train',)\" \"DATASETS.TEST\" \"('fracture_test',)\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/mtroym/EfficientDet.detectron2\n",
        "%cd \"EfficientDet.detectron2\"\n",
        "\n",
        "import logging\n",
        "import os\n",
        "from collections import OrderedDict\n",
        "\n",
        "import detectron2.utils.comm as comm\n",
        "import torch\n",
        "from detectron2.checkpoint import DetectionCheckpointer, PeriodicCheckpointer\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.data import (\n",
        "    MetadataCatalog,\n",
        "    build_detection_test_loader,\n",
        "    build_detection_train_loader,\n",
        ")\n",
        "from detectron2.engine import default_argument_parser, default_setup, launch\n",
        "from detectron2.evaluation import (\n",
        "\n",
        "    # CityscapesEvaluator,\n",
        "    COCOEvaluator,\n",
        "    COCOPanopticEvaluator,\n",
        "    DatasetEvaluators,\n",
        "    LVISEvaluator,\n",
        "    PascalVOCDetectionEvaluator,\n",
        "    SemSegEvaluator,\n",
        "    inference_on_dataset,\n",
        "    print_csv_format,\n",
        ")\n",
        "from detectron2.modeling import build_model\n",
        "from detectron2.solver import build_lr_scheduler, build_optimizer\n",
        "from detectron2.utils.events import (\n",
        "    CommonMetricPrinter,\n",
        "    EventStorage,\n",
        "    JSONWriter,\n",
        "    TensorboardXWriter,\n",
        ")\n",
        "from torch.nn.parallel import DistributedDataParallel\n",
        "\n",
        "from src.config import add_efficientdet_config\n",
        "from src.data import register_all_df2\n",
        "from src.modeling.efficientdet_heads import EfficientDetHead\n",
        "from src.data import DetDatasetMapper\n",
        "\n",
        "logger = logging.getLogger(\"detectron2\")\n",
        "\n",
        "\n",
        "# register_all_df2(\"/mnt/cephfs_new_wj/lab_ad_idea/maoyiming/data\")\n",
        "\n",
        "\n",
        "def get_evaluator(cfg, dataset_name, output_folder=None):\n",
        "    \"\"\"\n",
        "    Create evaluator(s) for a given dataset.\n",
        "    This uses the special metadata \"evaluator_type\" associated with each builtin dataset.\n",
        "    For your own dataset, you can simply create an evaluator manually in your\n",
        "    script and do not have to worry about the hacky if-else logic here.\n",
        "    \"\"\"\n",
        "    if output_folder is None:\n",
        "        output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
        "    evaluator_list = []\n",
        "    evaluator_type = MetadataCatalog.get(dataset_name).evaluator_type\n",
        "    if evaluator_type in [\"sem_seg\", \"coco_panoptic_seg\"]:\n",
        "        evaluator_list.append(\n",
        "            SemSegEvaluator(\n",
        "                dataset_name,\n",
        "                distributed=True,\n",
        "                num_classes=cfg.MODEL.SEM_SEG_HEAD.NUM_CLASSES,\n",
        "                ignore_label=cfg.MODEL.SEM_SEG_HEAD.IGNORE_VALUE,\n",
        "                output_dir=output_folder,\n",
        "            )\n",
        "        )\n",
        "    if evaluator_type in [\"coco\", \"coco_panoptic_seg\"]:\n",
        "        evaluator_list.append(COCOEvaluator(dataset_name, cfg, True, output_folder))\n",
        "    if evaluator_type == \"coco_panoptic_seg\":\n",
        "        evaluator_list.append(COCOPanopticEvaluator(dataset_name, output_folder))\n",
        "    # if evaluator_type == \"cityscapes\":\n",
        "    # assert (\n",
        "    #         torch.cuda.device_count() >= comm.get_rank()\n",
        "    # ), \"CityscapesEvaluator currently do not work with multiple machines.\"\n",
        "    # return CityscapesEvaluator(dataset_name)\n",
        "    if evaluator_type == \"pascal_voc\":\n",
        "        return PascalVOCDetectionEvaluator(dataset_name)\n",
        "    if evaluator_type == \"lvis\":\n",
        "        return LVISEvaluator(dataset_name, cfg, True, output_folder)\n",
        "    if len(evaluator_list) == 0:\n",
        "        raise NotImplementedError(\n",
        "            \"no Evaluator for the dataset {} with the type {}\".format(dataset_name, evaluator_type)\n",
        "        )\n",
        "    if len(evaluator_list) == 1:\n",
        "        return evaluator_list[0]\n",
        "    return DatasetEvaluators(evaluator_list)\n",
        "\n",
        "\n",
        "def do_test(cfg, model):\n",
        "    results = OrderedDict()\n",
        "    for dataset_name in cfg.DATASETS.TEST:\n",
        "        data_loader = build_detection_test_loader(cfg, dataset_name,\n",
        "                                                  mapper=DetDatasetMapper(cfg, is_train=False))\n",
        "        evaluator = get_evaluator(\n",
        "            cfg, dataset_name, os.path.join(cfg.OUTPUT_DIR, \"inference\", dataset_name)\n",
        "        )\n",
        "        results_i = inference_on_dataset(model, data_loader, evaluator)\n",
        "        results[dataset_name] = results_i\n",
        "        if comm.is_main_process():\n",
        "            logger.info(\"Evaluation results for {} in csv format:\".format(dataset_name))\n",
        "            print_csv_format(results_i)\n",
        "    if len(results) == 1:\n",
        "        results = list(results.values())[0]\n",
        "    return results\n",
        "\n",
        "\n",
        "def do_train(cfg, model, resume=False):\n",
        "    model.train()\n",
        "    optimizer = build_optimizer(cfg, model)\n",
        "    scheduler = build_lr_scheduler(cfg, optimizer)\n",
        "\n",
        "    checkpointer = DetectionCheckpointer(\n",
        "        model, cfg.OUTPUT_DIR, optimizer=optimizer, scheduler=scheduler\n",
        "    )\n",
        "    start_iter = (\n",
        "            checkpointer.resume_or_load(cfg.MODEL.WEIGHTS, resume=resume).get(\"iteration\", -1) + 1\n",
        "    )\n",
        "    max_iter = cfg.SOLVER.MAX_ITER\n",
        "\n",
        "    periodic_checkpointer = PeriodicCheckpointer(\n",
        "        checkpointer, cfg.SOLVER.CHECKPOINT_PERIOD, max_iter=max_iter\n",
        "    )\n",
        "\n",
        "    writers = (\n",
        "        [\n",
        "            CommonMetricPrinter(max_iter),\n",
        "            JSONWriter(os.path.join(cfg.OUTPUT_DIR, \"metrics.json\")),\n",
        "            TensorboardXWriter(cfg.OUTPUT_DIR),\n",
        "        ]\n",
        "        if comm.is_main_process()\n",
        "        else []\n",
        "    )\n",
        "\n",
        "    # compared to \"train_net.py\", we do not support accurate timing and\n",
        "    # precise BN here, because they are not trivial to implement\n",
        "    dataset_mapper = DetDatasetMapper(cfg, is_train=True)\n",
        "    data_loader = build_detection_train_loader(cfg, mapper=dataset_mapper)\n",
        "\n",
        "    logger.info(\"builded detection train loader by DatasetMapper\")\n",
        "    logger.info(\"Starting training from iteration {}\".format(start_iter))\n",
        "    with EventStorage(start_iter) as storage:\n",
        "        for data, iteration in zip(data_loader, range(start_iter, max_iter)):\n",
        "            iteration = iteration + 1\n",
        "            storage.step()\n",
        "\n",
        "            loss_dict = model(data)\n",
        "            losses = sum(loss_dict.values())\n",
        "            assert torch.isfinite(losses).all(), loss_dict\n",
        "\n",
        "            loss_dict_reduced = {k: v.item() for k, v in comm.reduce_dict(loss_dict).items()}\n",
        "            losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
        "            if comm.is_main_process():\n",
        "                storage.put_scalars(total_loss=losses_reduced, **loss_dict_reduced)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            losses.backward()\n",
        "            optimizer.step()\n",
        "            storage.put_scalar(\"lr\", optimizer.param_groups[0][\"lr\"], smoothing_hint=False)\n",
        "            scheduler.step(None)\n",
        "\n",
        "            if (\n",
        "                    cfg.TEST.EVAL_PERIOD > 0\n",
        "                    and iteration % cfg.TEST.EVAL_PERIOD == 0\n",
        "                    and iteration != max_iter\n",
        "            ):\n",
        "                do_test(cfg, model)\n",
        "                # Compared to \"train_net.py\", the test results are not dumped to EventStorage\n",
        "                comm.synchronize()\n",
        "\n",
        "            if iteration - start_iter > 5 and (iteration % 20 == 0 or iteration == max_iter):\n",
        "                for writer in writers:\n",
        "                    writer.write()\n",
        "            periodic_checkpointer.step(iteration)\n",
        "\n",
        "\n",
        "def setup():\n",
        "    \"\"\"\n",
        "    Create configs and perform basic setups.\n",
        "    \"\"\"\n",
        "    for d in [\"train\", \"test\"]:\n",
        "        DatasetCatalog.register(\"fracture_\" + d, lambda: get_fracture_dicts(\"data/FracAtlas\", d))\n",
        "        MetadataCatalog.get(\"fracture_\" + d).set(thing_classes=[\"fracture\"])\n",
        "    fracture_metadata = MetadataCatalog.get(\"fracture_train\")\n",
        "\n",
        "    cfg = get_cfg()\n",
        "    cfg = add_efficientdet_config(cfg)\n",
        "    # cfg.merge_from_file(args.config_file)\n",
        "    cfg.DATASETS.TRAIN = (\"fracture_train\",)\n",
        "    cfg.DATASETS.TEST = (\"fracture_test\",)\n",
        "    cfg.freeze()\n",
        "    # default_setup(\n",
        "    #     cfg # , args\n",
        "    # )  # if you don't like any of the default setup, write your own setup code\n",
        "    return cfg\n",
        "\n",
        "\n",
        "def main():\n",
        "    cfg = setup()\n",
        "\n",
        "    model = build_model()\n",
        "    # logger.info(\"Model:\\n{}\".format(model))\n",
        "    # if args.eval_only:\n",
        "    #     DetectionCheckpointer(model, save_dir=cfg.OUTPUT_DIR).resume_or_load(\n",
        "    #         cfg.MODEL.WEIGHTS, resume=args.resume\n",
        "    #     )\n",
        "    #     return do_test(cfg, model)\n",
        "\n",
        "    distributed = comm.get_world_size() > 1\n",
        "    if distributed:\n",
        "        model = DistributedDataParallel(\n",
        "            model, device_ids=[comm.get_local_rank()], broadcast_buffers=False,\n",
        "            find_unused_parameters=True\n",
        "        )\n",
        "    do_train(cfg, model)\n",
        "    return do_test(cfg, model)\n",
        "\n",
        "\n",
        "launch(\n",
        "    main,\n",
        "    1,\n",
        "    num_machines=1,\n",
        "    machine_rank=0,\n",
        "    dist_url=\"tcp://127.0.0.1:{}\".format(2**15 + 2**14 + hash(os.getuid()) % 2**14),\n",
        "    # args=(args,),\n",
        ")"
      ],
      "metadata": {
        "id": "Bl2LdhRhnPc0",
        "outputId": "a9b5df95-1bbe-4a86-de17-797397482738",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'EfficientDet.detectron2'...\n",
            "remote: Enumerating objects: 43, done.\u001b[K\n",
            "remote: Counting objects: 100% (43/43), done.\u001b[K\n",
            "remote: Compressing objects: 100% (32/32), done.\u001b[K\n",
            "remote: Total 43 (delta 7), reused 43 (delta 7), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (43/43), 35.27 KiB | 3.53 MiB/s, done.\n",
            "Resolving deltas: 100% (7/7), done.\n",
            "/content/EfficientDet.detectron2\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "default_setup() missing 1 required positional argument: 'args'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-8b895a972ac3>\u001b[0m in \u001b[0;36m<cell line: 223>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m launch(\n\u001b[0m\u001b[1;32m    224\u001b[0m     \u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/detectron2/engine/launch.py\u001b[0m in \u001b[0;36mlaunch\u001b[0;34m(main_func, num_gpus_per_machine, num_machines, machine_rank, dist_url, args, timeout)\u001b[0m\n\u001b[1;32m     82\u001b[0m         )\n\u001b[1;32m     83\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mmain_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-8b895a972ac3>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m     \u001b[0mcfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-8b895a972ac3>\u001b[0m in \u001b[0;36msetup\u001b[0;34m()\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATASETS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEST\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"fracture_test\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m     default_setup(\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0mcfg\u001b[0m \u001b[0;31m# , args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     )  # if you don't like any of the default setup, write your own setup code\n",
            "\u001b[0;31mTypeError\u001b[0m: default_setup() missing 1 required positional argument: 'args'"
          ]
        }
      ]
    }
  ]
}