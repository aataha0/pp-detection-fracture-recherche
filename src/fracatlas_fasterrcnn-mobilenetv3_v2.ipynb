{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch 2.1.0\n",
      "torchvision 0.16.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"pytorch\", torch.__version__)\n",
    "print(\"torchvision\", torchvision.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "from torchvision.datasets.utils import download_and_extract_archive, verify_str_arg\n",
    "from torchvision.ops import box_area\n",
    "\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "import collections\n",
    "import os\n",
    "from xml.etree.ElementTree import Element as ET_Element\n",
    "\n",
    "try:\n",
    "    from defusedxml.ElementTree import parse as ET_parse\n",
    "except ImportError:\n",
    "    from xml.etree.ElementTree import parse as ET_parse\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple\n",
    "\n",
    "\n",
    "class FracAtlasDetection(VisionDataset):\n",
    "    \"\"\"FracAtlas dataset.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        image_set: str = \"train\",\n",
    "        download: bool = False,\n",
    "        transform: Optional[Callable] = None,\n",
    "        target_transform: Optional[Callable] = None,\n",
    "        transforms: Optional[Callable] = None,\n",
    "    ):\n",
    "        super().__init__(root, transforms, transform, target_transform)\n",
    "\n",
    "        valid_image_sets = [\"test\", \"train\", \"val\"]\n",
    "        self.image_set = verify_str_arg(image_set, \"image_set\", valid_image_sets)\n",
    "\n",
    "        self.url = \"https://figshare.com/ndownloader/files/41725659\"\n",
    "        self.filename = \"FracAtlas.zip\"\n",
    "\n",
    "        if download:\n",
    "            download_and_extract_archive(self.url, self.root, filename=self.filename)\n",
    "        if not os.path.isdir(self.root):\n",
    "            raise RuntimeError(\n",
    "                \"Dataset not found or corrupted. You can use download=True to download it\"\n",
    "            )\n",
    "\n",
    "        splits_dir = os.path.join(self.root, \"Utilities\", \"Fracture Split\")\n",
    "        splits_f = os.path.join(splits_dir, image_set.rstrip(\"\\n\") + \".csv\")\n",
    "        with open(os.path.join(splits_f)) as f:\n",
    "            file_names = [\n",
    "                os.path.splitext(x.strip())[0]\n",
    "                for x in f.readlines()\n",
    "                if x.strip() != \"image_id\"\n",
    "            ]\n",
    "\n",
    "        image_dir = os.path.join(self.root, \"images\")\n",
    "        self.images = [os.path.join(image_dir, x + \".jpg\") for x in file_names]\n",
    "\n",
    "        target_dir = os.path.join(self.root, \"Annotations\", \"PASCAL VOC\")\n",
    "        self.targets = [os.path.join(target_dir, x + \".xml\") for x in file_names]\n",
    "\n",
    "        assert len(self.images) == len(self.targets)\n",
    "\n",
    "    @property\n",
    "    def annotations(self) -> List[str]:\n",
    "        return self.targets\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is a dictionary of the XML tree.\n",
    "        \"\"\"\n",
    "        img = Image.open(self.images[index]).convert(\"RGB\")\n",
    "        img = tv_tensors.Image(img)\n",
    "        voc_dict = self.parse_voc_xml(ET_parse(self.annotations[index]).getroot())\n",
    "        target = self.voc_dict_to_target(index, voc_dict, img)\n",
    "        # print(target)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img, target = self.transform(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images)\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_voc_xml(node: ET_Element) -> Dict[str, Any]:\n",
    "        voc_dict: Dict[str, Any] = {}\n",
    "        children = list(node)\n",
    "        if children:\n",
    "            def_dic: Dict[str, Any] = collections.defaultdict(list)\n",
    "            for dc in map(FracAtlasDetection.parse_voc_xml, children):\n",
    "                for ind, v in dc.items():\n",
    "                    def_dic[ind].append(v)\n",
    "            if node.tag == \"annotation\":\n",
    "                def_dic[\"object\"] = [def_dic[\"object\"]]\n",
    "            voc_dict = {\n",
    "                node.tag: {\n",
    "                    ind: v[0] if len(v) == 1 else v for ind, v in def_dic.items()\n",
    "                }\n",
    "            }\n",
    "        if node.text:\n",
    "            text = node.text.strip()\n",
    "            if not children:\n",
    "                voc_dict[node.tag] = text\n",
    "        return voc_dict\n",
    "\n",
    "    def voc_dict_to_target(self, index: int, item, img):\n",
    "        width = int(item[\"annotation\"][\"size\"][\"width\"])\n",
    "        height = int(item[\"annotation\"][\"size\"][\"height\"])\n",
    "\n",
    "        num_fractures = len(item[\"annotation\"][\"object\"])\n",
    "\n",
    "        boxes = [\n",
    "            (\n",
    "                int(obj[\"bndbox\"][\"xmin\"]),\n",
    "                int(obj[\"bndbox\"][\"ymin\"]),\n",
    "                int(obj[\"bndbox\"][\"xmax\"]),\n",
    "                int(obj[\"bndbox\"][\"ymax\"]),\n",
    "            )\n",
    "            for obj in item[\"annotation\"][\"object\"]\n",
    "        ]\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32).reshape(-1, 4)\n",
    "\n",
    "        # extract labels\n",
    "        labels = torch.ones((num_fractures,), dtype=torch.int64)\n",
    "\n",
    "        image_id = index\n",
    "        area = box_area(boxes)\n",
    "\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_fractures,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = tv_tensors.BoundingBoxes(boxes, format=\"xyxy\", canvas_size=img.shape[-2:])\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        return target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2 as T\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.ToImage())\n",
    "    if train:\n",
    "        transforms.append(T.RandomPhotometricDistort())\n",
    "        transforms.append(T.RandomZoomOut(fill={tv_tensors.Image: (0,0,0), \"others\": 0}))\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    transforms.append(T.ClampBoundingBoxes())\n",
    "    transforms.append(T.SanitizeBoundingBoxes())\n",
    "    transforms.append(T.ToDtype(torch.float32, scale=True))\n",
    "    return T.Compose(transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"lib/engine.py\"):\n",
    "    os.system(\n",
    "        \"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py\"\n",
    "    )\n",
    "if not os.path.exists(\"lib/utils.py\"):\n",
    "    os.system(\n",
    "        \"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\"\n",
    "    )\n",
    "if not os.path.exists(\"lib/transforms.py\"):\n",
    "    os.system(\n",
    "        \"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py\"\n",
    "    )\n",
    "if not os.path.exists(\"lib/coco_utils.py\"):\n",
    "    os.system(\n",
    "        \"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py\"\n",
    "    )\n",
    "if not os.path.exists(\"lib/coco_eval.py\"):\n",
    "    os.system(\n",
    "        \"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py\"\n",
    "    )\n",
    "\n",
    "# you also need to tweak them a bit to work with relative imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss_classifier': tensor(0.2171, grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.0130, grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.1639, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.0075, grad_fn=<DivBackward0>)}\n",
      "{'boxes': tensor([], size=(0, 4), grad_fn=<StackBackward0>), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([], grad_fn=<IndexBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models.detection import fasterrcnn_mobilenet_v3_large_fpn, FasterRCNN_MobileNet_V3_Large_FPN_Weights\n",
    "\n",
    "model = fasterrcnn_mobilenet_v3_large_fpn(weights=FasterRCNN_MobileNet_V3_Large_FPN_Weights.DEFAULT)\n",
    "dataset = FracAtlasDetection(\"../data/fracatlas\", image_set=\"train\", transform=get_transform(train=True))\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "  dataset,\n",
    "  batch_size=2,\n",
    "  shuffle=True,\n",
    "  # num_workers=4,\n",
    "  collate_fn=lambda batch: tuple(zip(*batch)),\n",
    ")\n",
    "\n",
    "# For Training\n",
    "images, targets = next(iter(data_loader))\n",
    "images = list(image for image in images)\n",
    "targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "output = model(images, targets)  # Returns losses and detections\n",
    "print(output)\n",
    "\n",
    "# For inference\n",
    "model.eval()\n",
    "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
    "predictions = model(x)  # Returns predictions\n",
    "print(predictions[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "Epoch: [0]  [  0/287]  eta: 0:18:30  lr: 0.000022  loss: 0.5173 (0.5173)  loss_classifier: 0.3322 (0.3322)  loss_box_reg: 0.0175 (0.0175)  loss_objectness: 0.1589 (0.1589)  loss_rpn_box_reg: 0.0087 (0.0087)  time: 3.8711  data: 0.0523  max mem: 596\n",
      "Epoch: [0]  [ 10/287]  eta: 0:06:31  lr: 0.000197  loss: 0.4367 (0.4571)  loss_classifier: 0.2899 (0.2970)  loss_box_reg: 0.0132 (0.0167)  loss_objectness: 0.1446 (0.1361)  loss_rpn_box_reg: 0.0087 (0.0073)  time: 1.4131  data: 0.5944  max mem: 890\n",
      "Epoch: [0]  [ 20/287]  eta: 0:05:36  lr: 0.000372  loss: 0.4177 (0.4146)  loss_classifier: 0.2193 (0.2524)  loss_box_reg: 0.0138 (0.0192)  loss_objectness: 0.1175 (0.1340)  loss_rpn_box_reg: 0.0086 (0.0090)  time: 1.1280  data: 0.5978  max mem: 1123\n",
      "Epoch: [0]  [ 30/287]  eta: 0:05:17  lr: 0.000546  loss: 0.2873 (0.3642)  loss_classifier: 0.1528 (0.2219)  loss_box_reg: 0.0146 (0.0175)  loss_objectness: 0.0905 (0.1155)  loss_rpn_box_reg: 0.0086 (0.0093)  time: 1.1376  data: 0.4890  max mem: 3599\n",
      "Epoch: [0]  [ 40/287]  eta: 0:05:08  lr: 0.000721  loss: 0.2090 (0.3200)  loss_classifier: 0.1297 (0.1933)  loss_box_reg: 0.0101 (0.0184)  loss_objectness: 0.0561 (0.0994)  loss_rpn_box_reg: 0.0070 (0.0090)  time: 1.2389  data: 0.5925  max mem: 3599\n",
      "Epoch: [0]  [ 50/287]  eta: 0:04:39  lr: 0.000896  loss: 0.1342 (0.2833)  loss_classifier: 0.0646 (0.1670)  loss_box_reg: 0.0062 (0.0176)  loss_objectness: 0.0407 (0.0898)  loss_rpn_box_reg: 0.0075 (0.0089)  time: 1.0897  data: 0.5529  max mem: 3599\n",
      "Epoch: [0]  [ 60/287]  eta: 0:04:27  lr: 0.001070  loss: 0.1071 (0.2548)  loss_classifier: 0.0441 (0.1483)  loss_box_reg: 0.0124 (0.0186)  loss_objectness: 0.0297 (0.0795)  loss_rpn_box_reg: 0.0071 (0.0083)  time: 1.0377  data: 0.4414  max mem: 3599\n",
      "Epoch: [0]  [ 70/287]  eta: 0:04:26  lr: 0.001245  loss: 0.0886 (0.2333)  loss_classifier: 0.0410 (0.1331)  loss_box_reg: 0.0167 (0.0186)  loss_objectness: 0.0231 (0.0732)  loss_rpn_box_reg: 0.0071 (0.0084)  time: 1.3618  data: 0.6474  max mem: 3599\n",
      "Epoch: [0]  [ 80/287]  eta: 0:04:11  lr: 0.001420  loss: 0.0886 (0.2158)  loss_classifier: 0.0393 (0.1219)  loss_box_reg: 0.0189 (0.0184)  loss_objectness: 0.0233 (0.0674)  loss_rpn_box_reg: 0.0069 (0.0082)  time: 1.3199  data: 0.6289  max mem: 3599\n",
      "Epoch: [0]  [ 90/287]  eta: 0:03:51  lr: 0.001594  loss: 0.0931 (0.2032)  loss_classifier: 0.0362 (0.1130)  loss_box_reg: 0.0146 (0.0183)  loss_objectness: 0.0297 (0.0633)  loss_rpn_box_reg: 0.0061 (0.0086)  time: 0.9701  data: 0.3796  max mem: 3599\n",
      "Epoch: [0]  [100/287]  eta: 0:03:48  lr: 0.001769  loss: 0.0794 (0.1916)  loss_classifier: 0.0309 (0.1059)  loss_box_reg: 0.0145 (0.0187)  loss_objectness: 0.0158 (0.0586)  loss_rpn_box_reg: 0.0065 (0.0084)  time: 1.2543  data: 0.5551  max mem: 3801\n",
      "Epoch: [0]  [110/287]  eta: 0:03:50  lr: 0.001944  loss: 0.0794 (0.1837)  loss_classifier: 0.0389 (0.1004)  loss_box_reg: 0.0188 (0.0191)  loss_objectness: 0.0150 (0.0554)  loss_rpn_box_reg: 0.0061 (0.0088)  time: 1.8951  data: 1.0624  max mem: 3801\n",
      "Epoch: [0]  [120/287]  eta: 0:03:40  lr: 0.002118  loss: 0.0857 (0.1752)  loss_classifier: 0.0370 (0.0947)  loss_box_reg: 0.0077 (0.0185)  loss_objectness: 0.0217 (0.0531)  loss_rpn_box_reg: 0.0077 (0.0089)  time: 1.8123  data: 1.0117  max mem: 3801\n",
      "Epoch: [0]  [130/287]  eta: 0:03:31  lr: 0.002293  loss: 0.0934 (0.1725)  loss_classifier: 0.0370 (0.0924)  loss_box_reg: 0.0122 (0.0206)  loss_objectness: 0.0223 (0.0508)  loss_rpn_box_reg: 0.0074 (0.0088)  time: 1.5855  data: 0.8307  max mem: 3801\n",
      "Epoch: [0]  [140/287]  eta: 0:03:25  lr: 0.002468  loss: 0.1202 (0.1700)  loss_classifier: 0.0564 (0.0901)  loss_box_reg: 0.0313 (0.0218)  loss_objectness: 0.0231 (0.0495)  loss_rpn_box_reg: 0.0065 (0.0086)  time: 1.8828  data: 1.1097  max mem: 3801\n",
      "Epoch: [0]  [150/287]  eta: 0:03:06  lr: 0.002642  loss: 0.1135 (0.1659)  loss_classifier: 0.0448 (0.0875)  loss_box_reg: 0.0256 (0.0225)  loss_objectness: 0.0218 (0.0474)  loss_rpn_box_reg: 0.0062 (0.0085)  time: 1.4593  data: 0.7497  max mem: 3801\n",
      "Epoch: [0]  [160/287]  eta: 0:02:48  lr: 0.002817  loss: 0.0974 (0.1632)  loss_classifier: 0.0458 (0.0857)  loss_box_reg: 0.0189 (0.0235)  loss_objectness: 0.0128 (0.0456)  loss_rpn_box_reg: 0.0063 (0.0084)  time: 0.8248  data: 0.2220  max mem: 3801\n",
      "Epoch: [0]  [170/287]  eta: 0:02:36  lr: 0.002992  loss: 0.0999 (0.1614)  loss_classifier: 0.0533 (0.0847)  loss_box_reg: 0.0189 (0.0239)  loss_objectness: 0.0200 (0.0445)  loss_rpn_box_reg: 0.0066 (0.0083)  time: 1.1248  data: 0.5031  max mem: 3801\n",
      "Epoch: [0]  [180/287]  eta: 0:02:22  lr: 0.003166  loss: 0.1084 (0.1616)  loss_classifier: 0.0557 (0.0842)  loss_box_reg: 0.0345 (0.0259)  loss_objectness: 0.0200 (0.0433)  loss_rpn_box_reg: 0.0057 (0.0082)  time: 1.3582  data: 0.6861  max mem: 3801\n",
      "Epoch: [0]  [190/287]  eta: 0:02:10  lr: 0.003341  loss: 0.1257 (0.1596)  loss_classifier: 0.0554 (0.0830)  loss_box_reg: 0.0325 (0.0261)  loss_objectness: 0.0181 (0.0423)  loss_rpn_box_reg: 0.0059 (0.0081)  time: 1.4179  data: 0.6463  max mem: 3801\n",
      "Epoch: [0]  [200/287]  eta: 0:01:55  lr: 0.003515  loss: 0.1161 (0.1575)  loss_classifier: 0.0503 (0.0814)  loss_box_reg: 0.0254 (0.0267)  loss_objectness: 0.0158 (0.0412)  loss_rpn_box_reg: 0.0068 (0.0081)  time: 1.2877  data: 0.5467  max mem: 3801\n",
      "Epoch: [0]  [210/287]  eta: 0:01:42  lr: 0.003690  loss: 0.1161 (0.1576)  loss_classifier: 0.0503 (0.0813)  loss_box_reg: 0.0334 (0.0280)  loss_objectness: 0.0158 (0.0404)  loss_rpn_box_reg: 0.0064 (0.0079)  time: 1.2639  data: 0.6093  max mem: 3801\n",
      "Epoch: [0]  [220/287]  eta: 0:01:30  lr: 0.003865  loss: 0.1437 (0.1571)  loss_classifier: 0.0634 (0.0806)  loss_box_reg: 0.0476 (0.0290)  loss_objectness: 0.0185 (0.0396)  loss_rpn_box_reg: 0.0064 (0.0079)  time: 1.5201  data: 0.8235  max mem: 3801\n",
      "Epoch: [0]  [230/287]  eta: 0:01:14  lr: 0.004039  loss: 0.1395 (0.1577)  loss_classifier: 0.0651 (0.0807)  loss_box_reg: 0.0445 (0.0304)  loss_objectness: 0.0171 (0.0388)  loss_rpn_box_reg: 0.0070 (0.0079)  time: 1.0584  data: 0.4500  max mem: 3801\n",
      "Epoch: [0]  [240/287]  eta: 0:01:01  lr: 0.004214  loss: 0.1353 (0.1563)  loss_classifier: 0.0651 (0.0797)  loss_box_reg: 0.0425 (0.0306)  loss_objectness: 0.0171 (0.0381)  loss_rpn_box_reg: 0.0063 (0.0078)  time: 0.8715  data: 0.3081  max mem: 3801\n",
      "Epoch: [0]  [250/287]  eta: 0:00:47  lr: 0.004389  loss: 0.1168 (0.1552)  loss_classifier: 0.0562 (0.0789)  loss_box_reg: 0.0334 (0.0312)  loss_objectness: 0.0176 (0.0374)  loss_rpn_box_reg: 0.0054 (0.0078)  time: 0.9855  data: 0.3917  max mem: 3801\n",
      "Epoch: [0]  [260/287]  eta: 0:00:35  lr: 0.004563  loss: 0.1054 (0.1539)  loss_classifier: 0.0493 (0.0781)  loss_box_reg: 0.0395 (0.0315)  loss_objectness: 0.0146 (0.0367)  loss_rpn_box_reg: 0.0050 (0.0077)  time: 1.4311  data: 0.7236  max mem: 3801\n",
      "Epoch: [0]  [270/287]  eta: 0:00:22  lr: 0.004738  loss: 0.1117 (0.1532)  loss_classifier: 0.0563 (0.0777)  loss_box_reg: 0.0283 (0.0319)  loss_objectness: 0.0108 (0.0359)  loss_rpn_box_reg: 0.0054 (0.0077)  time: 1.4741  data: 0.7565  max mem: 3801\n",
      "Epoch: [0]  [280/287]  eta: 0:00:09  lr: 0.004913  loss: 0.1328 (0.1527)  loss_classifier: 0.0660 (0.0774)  loss_box_reg: 0.0422 (0.0326)  loss_objectness: 0.0116 (0.0351)  loss_rpn_box_reg: 0.0063 (0.0076)  time: 1.2115  data: 0.5678  max mem: 3801\n",
      "Epoch: [0]  [286/287]  eta: 0:00:01  lr: 0.005000  loss: 0.1411 (0.1531)  loss_classifier: 0.0749 (0.0775)  loss_box_reg: 0.0517 (0.0333)  loss_objectness: 0.0111 (0.0348)  loss_rpn_box_reg: 0.0050 (0.0076)  time: 1.2379  data: 0.5940  max mem: 3801\n",
      "Epoch: [0] Total time: 0:06:14 (1.3045 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [ 0/61]  eta: 0:00:20  model_time: 0.3239 (0.3239)  evaluator_time: 0.0020 (0.0020)  time: 0.3323  data: 0.0028  max mem: 3801\n",
      "Test:  [60/61]  eta: 0:00:00  model_time: 0.1099 (0.1124)  evaluator_time: 0.0000 (0.0011)  time: 0.1233  data: 0.0067  max mem: 3801\n",
      "Test: Total time: 0:00:07 (0.1210 s / it)\n",
      "Averaged stats: model_time: 0.1099 (0.1124)  evaluator_time: 0.0000 (0.0011)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.02s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.059\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.254\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.012\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.062\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.060\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.099\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.137\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.137\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.121\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.218\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "Epoch: [1]  [  0/287]  eta: 0:07:13  lr: 0.005000  loss: 0.1745 (0.1745)  loss_classifier: 0.0865 (0.0865)  loss_box_reg: 0.0785 (0.0785)  loss_objectness: 0.0038 (0.0038)  loss_rpn_box_reg: 0.0057 (0.0057)  time: 1.5110  data: 0.5507  max mem: 3801\n",
      "Epoch: [1]  [ 10/287]  eta: 0:06:18  lr: 0.005000  loss: 0.1349 (0.1371)  loss_classifier: 0.0591 (0.0634)  loss_box_reg: 0.0362 (0.0452)  loss_objectness: 0.0159 (0.0214)  loss_rpn_box_reg: 0.0066 (0.0071)  time: 1.3656  data: 0.6240  max mem: 3801\n",
      "Epoch: [1]  [ 20/287]  eta: 0:06:24  lr: 0.005000  loss: 0.0998 (0.1314)  loss_classifier: 0.0461 (0.0608)  loss_box_reg: 0.0288 (0.0395)  loss_objectness: 0.0176 (0.0225)  loss_rpn_box_reg: 0.0070 (0.0086)  time: 1.4372  data: 0.7131  max mem: 3801\n",
      "Epoch: [1]  [ 30/287]  eta: 0:06:26  lr: 0.005000  loss: 0.1211 (0.1333)  loss_classifier: 0.0610 (0.0651)  loss_box_reg: 0.0419 (0.0410)  loss_objectness: 0.0158 (0.0197)  loss_rpn_box_reg: 0.0055 (0.0074)  time: 1.5771  data: 0.8819  max mem: 3801\n",
      "Epoch: [1]  [ 40/287]  eta: 0:05:52  lr: 0.005000  loss: 0.1568 (0.1502)  loss_classifier: 0.0818 (0.0770)  loss_box_reg: 0.0452 (0.0478)  loss_objectness: 0.0124 (0.0183)  loss_rpn_box_reg: 0.0051 (0.0072)  time: 1.4165  data: 0.7497  max mem: 3801\n",
      "Epoch: [1]  [ 50/287]  eta: 0:05:37  lr: 0.005000  loss: 0.1787 (0.1589)  loss_classifier: 0.0863 (0.0808)  loss_box_reg: 0.0730 (0.0540)  loss_objectness: 0.0113 (0.0170)  loss_rpn_box_reg: 0.0054 (0.0071)  time: 1.3009  data: 0.6301  max mem: 3801\n",
      "Epoch: [1]  [ 60/287]  eta: 0:05:07  lr: 0.005000  loss: 0.1563 (0.1621)  loss_classifier: 0.0799 (0.0819)  loss_box_reg: 0.0641 (0.0553)  loss_objectness: 0.0118 (0.0179)  loss_rpn_box_reg: 0.0048 (0.0069)  time: 1.1967  data: 0.5548  max mem: 3801\n",
      "Epoch: [1]  [ 70/287]  eta: 0:04:30  lr: 0.005000  loss: 0.1402 (0.1635)  loss_classifier: 0.0660 (0.0821)  loss_box_reg: 0.0577 (0.0578)  loss_objectness: 0.0113 (0.0169)  loss_rpn_box_reg: 0.0045 (0.0066)  time: 0.7917  data: 0.2219  max mem: 3801\n",
      "Epoch: [1]  [ 80/287]  eta: 0:04:20  lr: 0.005000  loss: 0.1402 (0.1626)  loss_classifier: 0.0660 (0.0810)  loss_box_reg: 0.0446 (0.0573)  loss_objectness: 0.0120 (0.0177)  loss_rpn_box_reg: 0.0041 (0.0067)  time: 0.9748  data: 0.3943  max mem: 3801\n",
      "Epoch: [1]  [ 90/287]  eta: 0:04:03  lr: 0.005000  loss: 0.1540 (0.1637)  loss_classifier: 0.0737 (0.0808)  loss_box_reg: 0.0406 (0.0588)  loss_objectness: 0.0156 (0.0173)  loss_rpn_box_reg: 0.0041 (0.0069)  time: 1.2121  data: 0.5885  max mem: 3801\n",
      "Epoch: [1]  [100/287]  eta: 0:03:52  lr: 0.005000  loss: 0.1384 (0.1618)  loss_classifier: 0.0613 (0.0790)  loss_box_reg: 0.0388 (0.0589)  loss_objectness: 0.0146 (0.0172)  loss_rpn_box_reg: 0.0042 (0.0067)  time: 1.1709  data: 0.5144  max mem: 3801\n",
      "Epoch: [1]  [110/287]  eta: 0:03:36  lr: 0.005000  loss: 0.1559 (0.1621)  loss_classifier: 0.0695 (0.0799)  loss_box_reg: 0.0555 (0.0594)  loss_objectness: 0.0122 (0.0163)  loss_rpn_box_reg: 0.0042 (0.0065)  time: 1.1566  data: 0.4932  max mem: 3801\n",
      "Epoch: [1]  [120/287]  eta: 0:03:16  lr: 0.005000  loss: 0.1629 (0.1655)  loss_classifier: 0.0802 (0.0802)  loss_box_reg: 0.0600 (0.0622)  loss_objectness: 0.0070 (0.0166)  loss_rpn_box_reg: 0.0050 (0.0065)  time: 0.8449  data: 0.2604  max mem: 3801\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: the launch timed out and was terminated\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\adnan\\Documents\\Projects\\xray-fracture-implementation\\src\\fracatlas_fasterrcnn-mobilenetv3_v2.ipynb Cell 6\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adnan/Documents/Projects/xray-fracture-implementation/src/fracatlas_fasterrcnn-mobilenetv3_v2.ipynb#W5sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adnan/Documents/Projects/xray-fracture-implementation/src/fracatlas_fasterrcnn-mobilenetv3_v2.ipynb#W5sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adnan/Documents/Projects/xray-fracture-implementation/src/fracatlas_fasterrcnn-mobilenetv3_v2.ipynb#W5sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     \u001b[39m# train for one epoch, printing every 10 iterations\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/adnan/Documents/Projects/xray-fracture-implementation/src/fracatlas_fasterrcnn-mobilenetv3_v2.ipynb#W5sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adnan/Documents/Projects/xray-fracture-implementation/src/fracatlas_fasterrcnn-mobilenetv3_v2.ipynb#W5sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     \u001b[39m# update the learning rate\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adnan/Documents/Projects/xray-fracture-implementation/src/fracatlas_fasterrcnn-mobilenetv3_v2.ipynb#W5sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     lr_scheduler\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\adnan\\Documents\\Projects\\xray-fracture-implementation\\src\\lib\\engine.py:57\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, optimizer, data_loader, device, epoch, print_freq, scaler)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[39mif\u001b[39;00m lr_scheduler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     55\u001b[0m         lr_scheduler\u001b[39m.\u001b[39mstep()\n\u001b[1;32m---> 57\u001b[0m     metric_logger\u001b[39m.\u001b[39mupdate(loss\u001b[39m=\u001b[39mlosses_reduced, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mloss_dict_reduced)\n\u001b[0;32m     58\u001b[0m     metric_logger\u001b[39m.\u001b[39mupdate(lr\u001b[39m=\u001b[39moptimizer\u001b[39m.\u001b[39mparam_groups[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m     60\u001b[0m \u001b[39mreturn\u001b[39;00m metric_logger\n",
      "File \u001b[1;32mc:\\Users\\adnan\\Documents\\Projects\\xray-fracture-implementation\\src\\lib\\utils.py:121\u001b[0m, in \u001b[0;36mMetricLogger.update\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m kwargs\u001b[39m.\u001b[39mitems():\n\u001b[0;32m    120\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(v, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m--> 121\u001b[0m         v \u001b[39m=\u001b[39m v\u001b[39m.\u001b[39;49mitem()\n\u001b[0;32m    122\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(v, (\u001b[39mfloat\u001b[39m, \u001b[39mint\u001b[39m))\n\u001b[0;32m    123\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmeters[k]\u001b[39m.\u001b[39mupdate(v)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: the launch timed out and was terminated\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "from lib.engine import train_one_epoch, evaluate\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(\"device:\", device)\n",
    "\n",
    "# use our dataset and defined transformations\n",
    "dataset = FracAtlasDetection('../data/fracatlas/', image_set=\"train\", transform=get_transform(train=True))\n",
    "dataset_test = FracAtlasDetection('../data/fracatlas/', image_set=\"test\", transform=get_transform(train=False))\n",
    "\n",
    "# split the dataset in train and test set\n",
    "# indices = torch.randperm(len(dataset)).tolist()\n",
    "# dataset = torch.utils.data.Subset(dataset, indices[:-80])\n",
    "# dataset_test = torch.utils.data.Subset(dataset_test, indices[-20:])\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    # num_workers=4,\n",
    "    collate_fn=lambda batch: tuple(zip(*batch)),\n",
    ")\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    # num_workers=4,\n",
    "    collate_fn=lambda batch: tuple(zip(*batch)),\n",
    ")\n",
    "\n",
    "model = fasterrcnn_mobilenet_v3_large_fpn(weights=FasterRCNN_MobileNet_V3_Large_FPN_Weights.DEFAULT)\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(\n",
    "    params,\n",
    "    lr=0.005,\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0005\n",
    ")\n",
    "\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=3,\n",
    "    gamma=0.1\n",
    ")\n",
    "\n",
    "# let's train it for 5 epochs\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    evaluate(model, data_loader_test, device=device)\n",
    "\n",
    "print(\"That's it!\")\n",
    "\n",
    "print(\"Saving\")\n",
    "torch.save(model, 'fracatlas_fasterrcnn-mobilenetv3_v2.pth')\n",
    "print(\"Saved\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
