{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGsH_MN9bpMQ",
        "outputId": "b6a45248-62b4-43a7-e513-34d16f80b9b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: torch 2.0.1+cu118\n",
            "Uninstalling torch-2.0.1+cu118:\n",
            "  Successfully uninstalled torch-2.0.1+cu118\n",
            "Found existing installation: torchvision 0.15.2+cu118\n",
            "Uninstalling torchvision-0.15.2+cu118:\n",
            "  Successfully uninstalled torchvision-0.15.2+cu118\n",
            "Found existing installation: torchaudio 2.0.2+cu118\n",
            "Uninstalling torchaudio-2.0.2+cu118:\n",
            "  Successfully uninstalled torchaudio-2.0.2+cu118\n",
            "Found existing installation: torchtext 0.15.2\n",
            "Uninstalling torchtext-0.15.2:\n",
            "  Successfully uninstalled torchtext-0.15.2\n",
            "Found existing installation: torchdata 0.6.1\n",
            "Uninstalling torchdata-0.6.1:\n",
            "  Successfully uninstalled torchdata-0.6.1\n",
            "Found existing installation: fastai 2.7.12\n",
            "Uninstalling fastai-2.7.12:\n",
            "  Successfully uninstalled fastai-2.7.12\n",
            "Collecting torchvision==0.16\n",
            "  Downloading torchvision-0.16.0-cp310-cp310-manylinux1_x86_64.whl (6.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch==2.1\n",
            "  Downloading torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.16) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.16) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.16) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.1)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.1)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.1)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m978.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.1)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.1)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.1)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.1)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.1)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.18.1 (from torch==2.1)\n",
            "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.1)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.1.0 (from torch==2.1)\n",
            "  Downloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.2.140-py3-none-manylinux1_x86_64.whl (20.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.2/20.2 MB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1) (1.3.0)\n",
            "Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.0.0\n",
            "    Uninstalling triton-2.0.0:\n",
            "      Successfully uninstalled triton-2.0.0\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.2.140 nvidia-nvtx-cu12-12.1.105 torch-2.1.0 torchvision-0.16.0 triton-2.1.0\n",
            "pytorch 2.1.0+cu121\n",
            "torchvision 0.16.0+cu121\n"
          ]
        }
      ],
      "source": [
        "%pip uninstall torch torchvision torchaudio torchtext torchdata fastai -y\n",
        "%pip install torchvision==0.16 torch==2.1 opencv-python matplotlib\n",
        "\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "print(\"pytorch\", torch.__version__)\n",
        "print(\"torchvision\", torchvision.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qkLcvgncbpMU"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import functional as F\n",
        "from torchvision.datasets.vision import VisionDataset\n",
        "from torchvision.datasets.utils import download_and_extract_archive, verify_str_arg\n",
        "from torchvision.ops import box_area\n",
        "from torchvision import tv_tensors\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import collections\n",
        "import os\n",
        "from xml.etree.ElementTree import Element as ET_Element\n",
        "\n",
        "try:\n",
        "    from defusedxml.ElementTree import parse as ET_parse\n",
        "except ImportError:\n",
        "    from xml.etree.ElementTree import parse as ET_parse\n",
        "from typing import Any, Callable, Dict, List, Optional, Tuple\n",
        "\n",
        "\n",
        "class FracAtlasDetection(VisionDataset):\n",
        "    \"\"\"FracAtlas dataset.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        root: str,\n",
        "        image_set: str = \"train\",\n",
        "        # download: bool = False,\n",
        "        transform: Optional[Callable] = None,\n",
        "    ):\n",
        "        super().__init__(root, transform=transform)\n",
        "\n",
        "        valid_image_sets = [\"test\", \"train\", \"val\"]\n",
        "        self.image_set = verify_str_arg(image_set, \"image_set\", valid_image_sets)\n",
        "\n",
        "        self.url = \"https://figshare.com/ndownloader/files/41725659\"\n",
        "        self.filename = \"fracatlas.zip\"\n",
        "\n",
        "        # if download:\n",
        "        if not os.path.isdir(\"data/FracAtlas\"):\n",
        "            os.makedirs(\"data\", exist_ok=True)\n",
        "            download_and_extract_archive(self.url, os.path.dirname(self.root), filename=self.filename, remove_finished=True)\n",
        "            for subdir in [\"Fractured\", \"Non_fractured\"]:\n",
        "                dirpath = os.path.join(self.root, \"images\")\n",
        "                subdirpath = os.path.join(dirpath, subdir)\n",
        "                for f in os.listdir(subdirpath):\n",
        "                    if not f.lower().endswith(\".jpg\"): continue\n",
        "                    os.rename(os.path.join(subdirpath, f), os.path.join(dirpath, f))\n",
        "                os.rmdir(subdirpath)\n",
        "            print(os.listdir(\"data\"))\n",
        "        if not os.path.isdir(self.root):\n",
        "            raise RuntimeError(\n",
        "                \"Dataset not found or corrupted. You can use download=True to download it\"\n",
        "            )\n",
        "\n",
        "        image_dir = os.path.join(root, \"images\")\n",
        "        target_dir = os.path.join(root, \"Annotations\", \"PASCAL VOC\")\n",
        "        all_images = [os.path.splitext(x)[0] for x in os.listdir(image_dir)]\n",
        "        \n",
        "        # 90% of images in train, in which 80% are actually training and 20% validation, and the last 10% in test\n",
        "        file_names = []\n",
        "        if image_set == \"test\":\n",
        "            file_names = all_images[int(0.9 * len(all_images)) :]\n",
        "        else:\n",
        "            all_file_names = all_images[: int(0.9 * len(all_images))]\n",
        "            if image_set == \"train\":\n",
        "                file_names = all_file_names[: int(0.8 * len(all_file_names))]\n",
        "            elif image_set == \"val\":\n",
        "                file_names = all_file_names[int(0.8 * len(all_file_names)) :]\n",
        "\n",
        "        file_names = [x for x in file_names if len(self.parse_voc_xml(ET_parse(os.path.join(target_dir, x + \".xml\")).getroot())[\"annotation\"][\"object\"]) <= 1]\n",
        "\n",
        "        self.images = [os.path.join(image_dir, x + \".jpg\") for x in file_names]\n",
        "        self.targets = [os.path.join(target_dir, x + \".xml\") for x in file_names]\n",
        "\n",
        "        assert len(self.images) == len(self.targets)\n",
        "\n",
        "    @property\n",
        "    def annotations(self) -> List[str]:\n",
        "        return self.targets\n",
        "\n",
        "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): Index\n",
        "\n",
        "        Returns:\n",
        "            tuple: (image, target) where target is a dictionary of the XML tree.\n",
        "        \"\"\"\n",
        "        img = Image.open(self.images[index]).convert(\"RGB\")\n",
        "        img = F.to_tensor(img)\n",
        "        voc_dict = self.parse_voc_xml(ET_parse(self.annotations[index]).getroot())\n",
        "        target = self.voc_dict_to_target(index, voc_dict)\n",
        "        # print(target)\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transform(img, target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.images)\n",
        "\n",
        "    @staticmethod\n",
        "    def parse_voc_xml(node: ET_Element) -> Dict[str, Any]:\n",
        "        voc_dict: Dict[str, Any] = {}\n",
        "        children = list(node)\n",
        "        if children:\n",
        "            def_dic: Dict[str, Any] = collections.defaultdict(list)\n",
        "            for dc in map(FracAtlasDetection.parse_voc_xml, children):\n",
        "                for ind, v in dc.items():\n",
        "                    def_dic[ind].append(v)\n",
        "            if node.tag == \"annotation\":\n",
        "                def_dic[\"object\"] = [def_dic[\"object\"]]\n",
        "            voc_dict = {\n",
        "                node.tag: {\n",
        "                    ind: v[0] if len(v) == 1 else v for ind, v in def_dic.items()\n",
        "                }\n",
        "            }\n",
        "        if node.text:\n",
        "            text = node.text.strip()\n",
        "            if not children:\n",
        "                voc_dict[node.tag] = text\n",
        "        return voc_dict\n",
        "\n",
        "    def voc_dict_to_target(self, index: int, item):\n",
        "        width = int(item[\"annotation\"][\"size\"][\"width\"])\n",
        "        height = int(item[\"annotation\"][\"size\"][\"height\"])\n",
        "\n",
        "        num_fractures = len(item[\"annotation\"][\"object\"])\n",
        "\n",
        "        boxes = [\n",
        "            (\n",
        "                int(obj[\"bndbox\"][\"xmin\"]),\n",
        "                int(obj[\"bndbox\"][\"ymin\"]),\n",
        "                int(obj[\"bndbox\"][\"xmax\"]),\n",
        "                int(obj[\"bndbox\"][\"ymax\"]),\n",
        "            )\n",
        "            for obj in item[\"annotation\"][\"object\"]\n",
        "        ]\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32).reshape(-1, 4)\n",
        "\n",
        "        # extract labels\n",
        "        labels = torch.ones((num_fractures,), dtype=torch.int64)\n",
        "\n",
        "        image_id = index\n",
        "        area = box_area(boxes)\n",
        "\n",
        "        # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((num_fractures,), dtype=torch.int64)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size=(height, width))\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        return target\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zaydHW2IbpMV"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import v2 as T\n",
        "\n",
        "\n",
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    # if train:\n",
        "    #     transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "    transforms.append(T.ToImage())\n",
        "    # transforms.append(T.Resize((256, 256), antialias=True))\n",
        "    transforms.append(T.ClampBoundingBoxes())\n",
        "    transforms.append(T.ToDtype(torch.float32, scale=True))\n",
        "    transforms.append(T.SanitizeBoundingBoxes())\n",
        "    # transforms.append(T.ToDtype(torch.float))\n",
        "    # transforms.append(T.ToPureTensor())\n",
        "    return T.Compose(transforms)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zdAf2dCObpMX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# os.makedirs(\"lib\", exist_ok=True)\n",
        "if not os.path.exists(\"engine.py\"):\n",
        "    os.system(\n",
        "        \"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py -O engine.py\"\n",
        "    )\n",
        "if not os.path.exists(\"utils.py\"):\n",
        "    os.system(\n",
        "        \"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py -O utils.py\"\n",
        "    )\n",
        "if not os.path.exists(\"transforms.py\"):\n",
        "    os.system(\n",
        "        \"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py -O transforms.py\"\n",
        "    )\n",
        "if not os.path.exists(\"coco_utils.py\"):\n",
        "    os.system(\n",
        "        \"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py -O coco_utils.py\"\n",
        "    )\n",
        "if not os.path.exists(\"coco_eval.py\"):\n",
        "    os.system(\n",
        "        \"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py -O coco_eval.py\"\n",
        "    )\n",
        "\n",
        "# you also need to tweak them a bit to work with relative imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVVZsro-bpMX",
        "outputId": "d03172ab-45bf-4f1e-8004-36109a120ec5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
            "100%|██████████| 160M/160M [00:01<00:00, 98.0MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/41725659/FracAtlas.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIYCQYOYV5JSSROOA/20231010/eu-west-1/s3/aws4_request&X-Amz-Date=20231010T220445Z&X-Amz-Expires=10&X-Amz-SignedHeaders=host&X-Amz-Signature=f74dab0ecd00fb4770150b569aae86c88c81d159846652ddc4dc11e7cd45e4a2 to data/fracatlas.zip\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 338412751/338412751 [00:10<00:00, 30818022.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/fracatlas.zip to data\n",
            "['FracAtlas']\n",
            "{'loss_classifier': tensor(0.2970, grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.1850, grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.0326, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.0058, grad_fn=<DivBackward0>)}\n",
            "{'boxes': tensor([], size=(0, 4), grad_fn=<StackBackward0>), 'labels': tensor([], dtype=torch.int64), 'scores': tensor([], grad_fn=<IndexBackward0>)}\n"
          ]
        }
      ],
      "source": [
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights\n",
        "from utils import collate_fn\n",
        "\n",
        "model = fasterrcnn_resnet50_fpn_v2(weights=FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT)\n",
        "dataset = FracAtlasDetection(\"data/FracAtlas\", image_set=\"train\", transform=get_transform(train=True))\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "  dataset,\n",
        "  batch_size=2,\n",
        "  shuffle=True,\n",
        "  num_workers=4,\n",
        "  collate_fn=collate_fn,\n",
        ")\n",
        "\n",
        "# For Training\n",
        "images, targets = next(iter(data_loader))\n",
        "images = list(image for image in images)\n",
        "targets = [{k: v for k, v in t.items()} for t in targets]\n",
        "output = model(images, targets)  # Returns losses and detections\n",
        "print(output)\n",
        "\n",
        "# For inference\n",
        "model.eval()\n",
        "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
        "predictions = model(x)  # Returns predictions\n",
        "print(predictions[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from torchvision.transforms import v2 as T\n",
        "\n",
        "def xyxy_to_xywh(xyxy):\n",
        "    \"\"\"\n",
        "    Convert XYXY format to XYWH format (x,y center point and width, height).\n",
        "    :param xyxy: [XMIN, YMIN, XMAX, YMAX]\n",
        "    :return: [X, Y, W, H]\n",
        "    \"\"\"\n",
        "    if len(xyxy) > 4:\n",
        "        raise ValueError('xyxy format: [x1, y1, x2, y2]')\n",
        "    x = xyxy[0]\n",
        "    y = xyxy[1]\n",
        "    w = xyxy[2] - xyxy[0]\n",
        "    h = xyxy[3] - xyxy[1]\n",
        "    return x, y, w, h\n",
        "\n",
        "\n",
        "for di in [random.randint(0, len(dataset)) for _ in range(3)]:\n",
        "    d = dataset[di]\n",
        "    # print(d)\n",
        "    im = T.ToPILImage()(d[0])\n",
        "\n",
        "    # Create figure and axes\n",
        "    fig, ax = plt.subplots()\n",
        " \n",
        "    # Display the image\n",
        "    ax.imshow(im)\n",
        "\n",
        "    boxes = d[1][\"boxes\"].tolist()\n",
        "\n",
        "    # print(boxes)\n",
        "\n",
        "    if len(boxes) > 0:\n",
        "        if type(boxes[0]) == list:\n",
        "            boxes = boxes[0]\n",
        "        x,y,w,h = xyxy_to_xywh(boxes)\n",
        "\n",
        "        # Create a Rectangle patch\n",
        "        rect = patches.Rectangle((x, y), w, h, linewidth=1, edgecolor='r', facecolor='none')\n",
        "\n",
        "        # Add the patch to the Axes\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPj51IP_bpMY",
        "outputId": "50a7dbaa-c596-4f16-dea3-4c3f95557d5f"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\adnan\\Documents\\Projects\\xray-fracture-implementation\\src\\fracatlas_fasterrcnn-resnet50_v4.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/adnan/Documents/Projects/xray-fracture-implementation/src/fracatlas_fasterrcnn-resnet50_v4.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmath\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/adnan/Documents/Projects/xray-fracture-implementation/src/fracatlas_fasterrcnn-resnet50_v4.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/adnan/Documents/Projects/xray-fracture-implementation/src/fracatlas_fasterrcnn-resnet50_v4.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mengine\u001b[39;00m \u001b[39mimport\u001b[39;00m train_one_epoch, evaluate\n",
            "\u001b[1;32mc:\\Users\\adnan\\Documents\\Projects\\xray-fracture-implementation\\src\\fracatlas_fasterrcnn-resnet50_v4.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/adnan/Documents/Projects/xray-fracture-implementation/src/fracatlas_fasterrcnn-resnet50_v4.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmath\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/adnan/Documents/Projects/xray-fracture-implementation/src/fracatlas_fasterrcnn-resnet50_v4.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/adnan/Documents/Projects/xray-fracture-implementation/src/fracatlas_fasterrcnn-resnet50_v4.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mengine\u001b[39;00m \u001b[39mimport\u001b[39;00m train_one_epoch, evaluate\n",
            "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mc:\\Users\\adnan\\Documents\\Projects\\xray-fracture-implementation\\.conda\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[0;32m   2067\u001b[0m             from_this_thread\u001b[39m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[0;32m   2069\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_threads_suspended_single_notification\u001b[39m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[1;32m-> 2070\u001b[0m         keep_suspended \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n\u001b[0;32m   2072\u001b[0m frames_list \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   2074\u001b[0m \u001b[39mif\u001b[39;00m keep_suspended:\n\u001b[0;32m   2075\u001b[0m     \u001b[39m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\adnan\\Documents\\Projects\\xray-fracture-implementation\\.conda\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[0;32m   2103\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_input_hook()\n\u001b[0;32m   2105\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_internal_commands()\n\u001b[1;32m-> 2106\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.01\u001b[39;49m)\n\u001b[0;32m   2108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[39mstr\u001b[39m(\u001b[39mid\u001b[39m(frame)))\n\u001b[0;32m   2110\u001b[0m \u001b[39m# process any stepping instructions\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from engine import train_one_epoch, evaluate\n",
        "from utils import collate_fn\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(\"device:\", device)\n",
        "\n",
        "# use our dataset and defined transformations\n",
        "dataset = FracAtlasDetection('data/FracAtlas', image_set=\"train\", transform=get_transform(train=True))\n",
        "dataset_test = FracAtlasDetection('data/FracAtlas', image_set=\"test\", transform=get_transform(train=False))\n",
        "dataset_val = FracAtlasDetection('data/FracAtlas', image_set=\"val\", transform=get_transform(train=False))\n",
        "\n",
        "# split the dataset in train and test set\n",
        "# indices = torch.randperm(len(dataset)).tolist()\n",
        "# dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
        "# dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
        "\n",
        "# define training and validation data loaders\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=2,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        "    collate_fn=collate_fn,\n",
        ")\n",
        "\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "    dataset_test,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    num_workers=4,\n",
        "    collate_fn=collate_fn,\n",
        ")\n",
        "\n",
        "data_loader_val = torch.utils.data.DataLoader(\n",
        "    dataset_val,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    num_workers=4,\n",
        "    collate_fn=collate_fn,\n",
        ")\n",
        "\n",
        "model = fasterrcnn_resnet50_fpn_v2(weights=FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT)\n",
        "\n",
        "# move model to the right device\n",
        "model.to(device)\n",
        "\n",
        "# construct an optimizer\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(\n",
        "    params,\n",
        "    lr=0.005,\n",
        "    momentum=0.9,\n",
        "    weight_decay=0.0005\n",
        ")\n",
        "\n",
        "# and a learning rate scheduler\n",
        "# lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "#     optimizer,\n",
        "#     step_size=3,\n",
        "#     gamma=0.1\n",
        "# )\n",
        "\n",
        "# let's train it for 5 epochs\n",
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # train for one epoch, printing every 10 iterations\n",
        "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
        "    # update the learning rate\n",
        "    # lr_scheduler.step()\n",
        "    # evaluate on the test dataset\n",
        "    evaluate(model, data_loader_val, device=device)\n",
        "\n",
        "print(\"Test\")\n",
        "coco_test_evaluator = evaluate(model, data_loader_test, device=device)\n",
        "for coco_eval in coco_test_evaluator.coco_eval.values():\n",
        "    print(\"Precision/test\", coco_eval.eval[\"precision\"])\n",
        "\n",
        "print(\"That's it!\")\n",
        "\n",
        "print(\"Saving\")\n",
        "torch.save(model, 'fracatlas_fasterrcnn-resnet50_v4.pth')\n",
        "print(\"Saved\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
